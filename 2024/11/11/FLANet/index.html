<html lang="en"><head><meta charset="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><meta name="viewport" content="width=device-width, initial-scale=1"/><meta name="description" content="false"/><title>FLANet
-
Bamboo
-

</title><link rel="icon" href="/img/favicon.ico"/>
<link rel="stylesheet" href="/css/style.css">

<link rel="stylesheet" href="/css/helpers.css">

<script src="/js/clipboard/clipboard.min.js"></script>


<script src="/js/bootstrap.js"></script>

<script async="async" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><meta name="generator" content="Hexo 7.2.0"></head><body><div class="loading-wrapper" data-loading="data-loading"><div class="loading"><span></span><span></span><span></span></div></div><div class="page" data-filter="data-filter"><div class="head" data-show="data-show"><header class="head-header"><div class="head-author"><a class="head-author-link" href="/">Bamboo</a></div><div class="head-right"><button class="bar-wrap" id="bar-wrap-toggle" title="menu button"><span class="bar"></span><span class="bar"></span><span class="bar"></span></button><div class="head-item"><a class="search-button head-item-link"><span>Search</span>
<i class="icon icon-search"></i></a></div><div class="head-item"><a class="head-item-link" href="/about">关于</a></div></div></header>
<div class="menubar-head" id="menubar"><ul class="menubar-ul"><li class="menubar-item"><i class="icon icon-chevron-right"></i>
<a class="menubar-link" href="/categories/Posts/">Posts1</a></li><li class="menubar-item"><i class="icon icon-chevron-right"></i>
<a class="menubar-link" href="/categories/Posts2/">Posts2</a></li><li class="menubar-item"><i class="icon icon-chevron-right"></i>
<a class="menubar-link" href="/categories/Posts3/">Posts3</a></li><li class="menubar-item" data-border="data-border"></li><li class="menubar-item"><i class="icon icon-archive"></i>
<a class="menubar-link" href="/archives">Archives</a></li><li class="menubar-item"><i class="icon icon-tags"></i>
<a class="menubar-link" href="/tags">Tags</a></li><li class="menubar-item" data-border="data-border"></li><li class="menubar-item"><a class="menubar-link" href="/about"><span>关于</span></a></li></ul><div class="menu-search-box search-button"><div>Search</div>
<i class="icon icon-search"></i></div></div></div><div class="main" data-page="post"><article class="post" id="post"><header class="post-head"><h1 class="post-title"><a class="title" href="/2024/11/11/FLANet/">FLANet</a></h1></header><div class="post-meta"><div class="post-date"><time class="post-time" itemprop="datePublished" title="2024-11-11 19:11:51" datetime="2024-11-11T11:11:51.000Z">2024-11-11</time></div>|
<div class="post-tag"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" rel="tag">论文阅读</a></li></ul></div>
<div class="post-visit"><span id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv"></span>hits</span></div></div><div class="post-info"><div class="post-word-count">This article contains 1,391 words.</div>
<div class="post-cc">Copyright: 署名

|
<a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/2.5/cn/">CC BY-NC-SA 2.5 CN</a></div></div><div class="article-entry" itemprop="articleBody"><p>全称:Fully Attentional Network for Semantic Segmentation</p>
<p>全注意力模块代码:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FullyAttentionalBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, plane, norm_layer=SyncBatchNorm</span>):</span><br><span class="line">        <span class="built_in">super</span>(FullyAttentionalBlock, self).__init__()</span><br><span class="line">        <span class="comment">#用于在高度和宽度方向上编码特征的线性层</span></span><br><span class="line">        self.conv1 = nn.Linear(plane, plane)</span><br><span class="line">        self.conv2 = nn.Linear(plane, plane)</span><br><span class="line">        </span><br><span class="line">        self.conv = nn.Sequential(nn.Conv2d(plane, plane, <span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">                                  norm_layer(plane),</span><br><span class="line">                                  nn.ReLU())</span><br><span class="line"></span><br><span class="line">        self.softmax = nn.Softmax(dim=-<span class="number">1</span>)</span><br><span class="line">        self.gamma = nn.Parameter(torch.zeros(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        batch_size, _, height, width = x.size()</span><br><span class="line"></span><br><span class="line">        feat_h = x.permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>).contiguous().view(batch_size * width, -<span class="number">1</span>, height)</span><br><span class="line">        feat_w = x.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>).contiguous().view(batch_size * height, -<span class="number">1</span>, width)</span><br><span class="line">        encode_h = self.conv1(F.avg_pool2d(x, [<span class="number">1</span>, width]).view(batch_size, -<span class="number">1</span>, height).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>).contiguous())</span><br><span class="line">        encode_w = self.conv2(F.avg_pool2d(x, [height, <span class="number">1</span>]).view(batch_size, -<span class="number">1</span>, width).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>).contiguous())</span><br><span class="line"></span><br><span class="line">        energy_h = torch.matmul(feat_h, encode_h.repeat(width, <span class="number">1</span>, <span class="number">1</span>))<span class="comment">#通过矩阵乘法计算高度方向的能量分数。</span></span><br><span class="line">        energy_w = torch.matmul(feat_w, encode_w.repeat(height, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        full_relation_h = self.softmax(energy_h)  <span class="comment"># [b*w, c, c]   获得高度方向的注意力权重</span></span><br><span class="line">        full_relation_w = self.softmax(energy_w)</span><br><span class="line"></span><br><span class="line">        full_aug_h = torch.bmm(full_relation_h, feat_h).view(batch_size, width, -<span class="number">1</span>, height).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>)<span class="comment">#使用批量矩阵乘法 (torch.bmm) 将高度方向的注意力应用于 feat_h</span></span><br><span class="line">        full_aug_w = torch.bmm(full_relation_w, feat_w).view(batch_size, height, -<span class="number">1</span>, width).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">        out = self.gamma * (full_aug_h + full_aug_w) + x </span><br><span class="line">        out = self.conv(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>

<h2 id="abstract"><a href="#abstract" class="headerlink" title="abstract"></a>abstract</h2><p>在原来可以通过压缩空间维度或通过压缩通道的相似图来描述沿通道或空间维度的特征关系，但是这样会沿着其他维度压缩特征依赖性，就会导致注意力缺失，对小类别的结果较差或者大对象内分割不一致。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>在FLANet中，将空间和通道注意力编码到在单个相似图中，同时保持高计算效率。</p>
<p><img src="/images/image-20241111113854239.png" alt="image-20241111113854239"></p>
<p>空间NL可以增强细节的辨别力，而通道NL则有利于保持大对象内部的语义一致性空间NL可以增强细节的辨别力，而通道NL则有利于保持大对象内部的语义一致性。</p>
<p>而且单纯把两个NL模块堆叠起来使用还是会出现注意力缺失的问题，注意力缺失问题会损害特征表示能力，并且不能通过简单地堆叠不同的NL块来解决。，但是FLA解决了这一问题。</p>
<p>FLA基本思想:在计算通道注意力图时利用全局上下文信息来接收空间响应，从而能够在单个注意力单元中实现充分的注意力，并且具有较高的计算效率。具体来说，我们首先使每个空间位置能够从具有相同水平和垂直坐标的全局上下文中获取特征响应。其次，我们使用自注意力机制来捕获任意两个通道图之间的完全注意力相似性以及相关的空间位置,最后，通过整合所有通道图和相关全局线索之间的特征，使用生成的完全注意相似性来重新加权每个通道图。</p>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p><img src="/images/image-20241111114639873.png" alt="image-20241111114639873"></p>
<p>为了避免增加额外的计算负担，我们尝试利用全局平均池化结果作为全局上下文先验，将空间交互引入通道NL机制。</p>
<p>具体计算过程：</p>
<p>给定输入特征图 Fin ∈ RC×H×W ，其中 C 是通道数，H 和 W 是输入张量的空间维度。首先，我们将 Fin 输入到底部的两个平行路径（即构建），每个路径都包含一个全局平均池化层，后面跟着一个线性层。在选择池化窗口的大小时，我们考虑了以下两个方面。首先，为了获得更丰富的全局上下文先验，我们选择在高度和宽度方向上使用不相等的全局池化大小而不是内核像3×3这样的窗口。其次，为了确保每个空间位置都与具有相同水平或垂直坐标的相应全局先验连接，即在计算通道关系时保持空间一致性，我们选择保留一维的长度持续的。因此，我们在这两个路径中分别采用大小为 H × 1 和 1 × W 的池化窗口。这给出 ˆ Qw ∈ RC×1×W 和 ˆ Qh ∈ RC×H×1。之后，我们重复 ^ Qw 和 ^ Qh 形成全局特征 Qw ∈ RC×H×W 和 Qh ∈ RC×H×W 。请注意，Qw和Qh分别表示水平和垂直方向上的全局先验，它们将用于实现相应维度上的空间交互。此外，我们沿 H 维度切割 Qw，从中我们可以生成一组大小为 RC×W 的 H 切片。同样，我们沿着W维度切割Qh。然后我们合并这两个组以形成最终的全局上下文 Q ∈ R(H+W )×C×S。剪切和合并操作如图3 所示。同时，我们沿 H 维度切割输入特征 Fin，产生一组大小为 RC×W 的 H 切片。同样，我们沿着 W 维度进行此操作。与 Q 的合并过程一样，这两组被整合形成特征 K ∈ R(H+W )×S×C 。以同样的方式，我们可以生成特征图V ∈ R(H+W )×C×S。</p>
<h3 id="贡献"><a href="#贡献" class="headerlink" title="贡献:"></a>贡献:</h3><ul>
<li>发现非局部自注意力方法中存在注意力缺失问题，这会损害特征表示的完整性。 </li>
<li>我们将自注意力机制重新表述为完全注意力方式，以生成密集且全面的特征依赖关系，从而有效且高效地解决注意力缺失问题.</li>
<li>实验广泛。</li>
</ul>
<h3 id="不足"><a href="#不足" class="headerlink" title="不足:"></a>不足:</h3><p>极高的计算量限制了其实际应用.</p>
</div></article><aside class="post-widget"><h4>In this article</h4><nav class="post-toc-wrap" id="post-toc"><ol class="post-toc"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#abstract"><span class="post-toc-number">1.</span> <span class="post-toc-text">abstract</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Introduction"><span class="post-toc-number">2.</span> <span class="post-toc-text">Introduction</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Method"><span class="post-toc-number">3.</span> <span class="post-toc-text">Method</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#%E8%B4%A1%E7%8C%AE"><span class="post-toc-number">3.1.</span> <span class="post-toc-text">贡献:</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#%E4%B8%8D%E8%B6%B3"><span class="post-toc-number">3.2.</span> <span class="post-toc-text">不足:</span></a></li></ol></li></ol></nav></aside></div><footer class="footer-nav"><div class="footer"><div class="back-top" id="back-top" title="Back to top"><i class="icon icon-chevron-bar-up"></i></div><div class="footer-content"><div><span id="busuanzi_container_site_pv"><span id="busuanzi_value_site_pv">?</span>
PV
</span><span id="busuanzi_container_site_uv"><span id="busuanzi_value_site_uv">?</span>
UV</span></div>

Copyright &copy;
2020<span class="time-divide">-</span>2025
Bamboo.

Power by
<a href="https://hexo.io/" target="_blank" rel="external nofollow">Hexo</a>
and
<a href="https://github.com/Cerallin/hexo-theme-yuzu" target="_blank" rel="external nofollow" title="v3.2.5">Theme Yuzu</a>.</div></div></footer>
<script>window.config = {
  url_root: '/',
  meta_path: 'meta.json',
};
</script>
<script src="/js/theme/back-to-top.js"></script>


<script src="/js/theme/clipboard.js"></script>


<script src="/js/theme/loading.js"></script>


<script src="/js/theme/navbar.js"></script>

<script src="/js/theme/search.js"></script>

<script src="/js/theme/toc.js"></script>
<script>window.onload = function () {
  for (const moduleName in Theme) {
    const module = Theme[moduleName];
    module.register();
  }
};</script></div><div class="search-modal" id="search-modal"><div class="card"><div class="card-head"><div class="search-box"><input class="search-input" id="search-input" placeholder="search"/><div class="search-button" id="search-button"><div class="icon icon-search"></div></div></div><div class="close-button"><div class="icon icon-x"></div></div></div><div class="card-body"><div class="search-count">search.total<span id="search-count-num">0</span>search result(s) in total.</div><div class="search-result" id="search-result"></div></div></div></div></body></html>