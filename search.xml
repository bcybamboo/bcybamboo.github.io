<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Yolov10</title>
    <url>/2024/05/26/Yolov10%E5%B0%8F%E7%9F%A5%E8%AF%86/</url>
    <content><![CDATA[<p> 今天突然发现都出了YOLOv10了，技术真的变更的好快，以下是本人学习的一些小知识。</p>
<p>为了提升性能效率边界，这一次改进的是后处理和模型架构。</p>
<p>在原来，YOLO后处理依赖着非极大值抑制（NMS），性能对NMS的超参数敏感，阻碍了YOLO的端到端部署，产生了一定的推理延迟。</p>
<p>增强特征提取能力可以考虑以下模块：DarkNet、CSPNet、EfficientRep和ELAN等</p>
<p>DarkNet是yolov3里面的backbone，主要是由重复堆叠下采样卷积+n*残差块组成。主要是由cnn卷积核提取特征。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/14b426a4f6b44003b64dd82a14daa2d7.png"
                      alt="图片"
                ></p>
<p>CSPNet的主要目的是使该体系结构能够实现更丰富的梯度组合，同时减少计算量。</p>
<p>增强多尺度特征融合：PAN、BiC、GD和RepGFPN等</p>
<p>模型缩放策略和重新参数化技术</p>
]]></content>
      <tags>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>进程调度算法</title>
    <url>/2024/08/28/%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<h2 id="先来先服务调度算法FCFS"><a href="#先来先服务调度算法FCFS" class="headerlink" title="先来先服务调度算法FCFS"></a>先来先服务调度算法FCFS</h2><p><strong>每次从就绪队列选择最先进入队列的进程，然后一直运行，直到进程退出或被阻塞，才会继续从队列中选择第一个进程接着运行。</strong></p>
<p>对短作业不利。</p>
<p>适用于 CPU 繁忙型作业的系统，而不适用于 I&#x2F;O 繁忙型作业的系统。</p>
<h2 id="最短作业优先调度算法SJF"><a href="#最短作业优先调度算法SJF" class="headerlink" title="最短作业优先调度算法SJF"></a>最短作业优先调度算法SJF</h2><p><strong>优先选择运行时间最短的进程来运行</strong>，这有助于提高系统的吞吐量。</p>
<p>对长作业不利，被推迟到很后面。</p>
<h2 id="高响应比优先调度算法HRRN"><a href="#高响应比优先调度算法HRRN" class="headerlink" title="高响应比优先调度算法HRRN"></a>高响应比优先调度算法HRRN</h2><p>进程调度先运行响应比优先级最高的。<br>$$<br>优先权&#x3D; \frac { 等待时间+要求服务时间 }{ 要求服务时间 }<br>$$<br>兼顾长作业和短作业。</p>
<h2 id="时间片轮转调度算法RR"><a href="#时间片轮转调度算法RR" class="headerlink" title="时间片轮转调度算法RR"></a>时间片轮转调度算法RR</h2><p><strong>使用最广</strong></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20240828124832086.png"
                      alt="image-20240828124832086"
                ></p>
<p>每个进程被分配一个时间段，称为时间片，即在该时间内可以为该进程运行。</p>
<ul>
<li>如果时间片用完，进程还在运行，那么将会把此进程从 CPU 释放出来，并把 CPU 分配另外一个进程；</li>
<li>如果该进程在时间片结束前阻塞或结束，则 CPU 立即进行切换；</li>
</ul>
<p>另外，时间片的长度就是一个很关键的点：</p>
<ul>
<li>如果时间片设得太短会导致过多的进程上下文切换，降低了 CPU 效率；</li>
<li>如果设得太长又可能引起对短作业进程的响应时间变长。将</li>
</ul>
<p>通常时间片设为 <code>20ms~50ms</code> 通常是一个比较合理的折中值。</p>
]]></content>
      <tags>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title>about_conda</title>
    <url>/2024/07/09/about-conda/</url>
    <content><![CDATA[<p>记录一下常用的conda命令</p>
<p>查看版本</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda --version</span><br></pre></td></tr></table></figure></div>

<p>查看环境配置</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda config --show</span><br></pre></td></tr></table></figure></div>

<p>更新conda</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda update conda</span><br></pre></td></tr></table></figure></div>

<p>创建虚拟环境</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda create -n env_name python=3.9</span><br></pre></td></tr></table></figure></div>

<p>查看虚拟环境</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda env list</span><br><span class="line">conda info -e</span><br><span class="line">conda info --envs</span><br></pre></td></tr></table></figure></div>

<p>激活虚拟环境</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda activate env_name</span><br></pre></td></tr></table></figure></div>

<p>退出虚拟环境</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda deactivate</span><br></pre></td></tr></table></figure></div>

<p>删除虚拟环境</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda remove --name env_name --all</span><br></pre></td></tr></table></figure></div>

<p>删除虚拟环境中的某些包</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda remove --name env_name  package_name</span><br></pre></td></tr></table></figure></div>

<p>查看环境中有的包</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda list</span><br></pre></td></tr></table></figure></div>

<p>克隆虚拟环境</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda create -n BBB --clone AAA</span><br></pre></td></tr></table></figure></div>

<p>在本地的conda环境AAA，克隆给新环境BBB</p>
]]></content>
      <tags>
        <tag>工具使用</tag>
      </tags>
  </entry>
  <entry>
    <title>如何读论文</title>
    <url>/2024/02/16/begin/</url>
    <content><![CDATA[<h1 id="如何读论文"><a href="#如何读论文" class="headerlink" title="如何读论文"></a>如何读论文</h1><p>从头开始看一篇论文所花的时间太久了，而且记忆不深，就没什么效果（对我来说），所以在参考了学长的观点后，我觉得可以从以下结构开始阅读：</p>
<table>
<thead>
<tr>
<th align="center">原来结构</th>
<th align="center">推荐结构</th>
</tr>
</thead>
<tbody><tr>
<td align="center">Abstract</td>
<td align="center">Abstract(摘要)</td>
</tr>
<tr>
<td align="center">Introduction</td>
<td align="center">Discussion（讨论）</td>
</tr>
<tr>
<td align="center">Methods</td>
<td align="center">Introduction（导言）</td>
</tr>
<tr>
<td align="center">Results</td>
<td align="center">Results（结果）</td>
</tr>
<tr>
<td align="center">Discussion</td>
<td align="center">Methods（方法）</td>
</tr>
</tbody></table>
<h2 id="Abstract-摘要"><a href="#Abstract-摘要" class="headerlink" title="Abstract(摘要)"></a>Abstract(摘要)</h2><p>关注四个信息：</p>
<ul>
<li>研究目的（为什么要研究）</li>
<li>方法（如何研究）</li>
<li>结果（发现了什么）</li>
<li>结论（它意味着什么）</li>
</ul>
<h2 id="Discussion（讨论）"><a href="#Discussion（讨论）" class="headerlink" title="Discussion（讨论）"></a>Discussion（讨论）</h2><p>内容一般是：</p>
<ul>
<li>明确回答introduction中提出的问题</li>
<li>解释结果如何支持结论</li>
</ul>
<p>看看自己是否理解和相信作者的观点</p>
<h2 id="Introduction（导言）"><a href="#Introduction（导言）" class="headerlink" title="Introduction（导言）"></a>Introduction（导言）</h2><p>作用：</p>
<ul>
<li>激发我们对主题的兴趣</li>
<li>将文章置于大背景中</li>
</ul>
<p>一般来说，先引导作者从一般问题（对主题的已知了解）到具体问题（对主题的未知了解），再到重点问题（作者提出的问题）。所以要介绍之前的作品以及这些作品与该主题的关系。</p>
<p>想想作者为什么要做这个研究，研究的问题和讨论的问题是否一致</p>
<h2 id="Results（结果）"><a href="#Results（结果）" class="headerlink" title="Results（结果）"></a>Results（结果）</h2><p>内容一般是：</p>
<ul>
<li>作者的发现</li>
<li>关键数据，通常用图表显示</li>
</ul>
<p>问问自己所收集的数据是否适合回答所研究的问题</p>
<h2 id="Methods（方法）"><a href="#Methods（方法）" class="headerlink" title="Methods（方法）"></a>Methods（方法）</h2><ul>
<li>做了哪些实验来回答引言中提出的问题</li>
</ul>
<h2 id="如何找到文章的要点"><a href="#如何找到文章的要点" class="headerlink" title="如何找到文章的要点"></a>如何找到文章的要点</h2><p>主要地方包括：</p>
<ul>
<li><p>文章标题</p>
</li>
<li><p>Abstract</p>
</li>
<li><p>Keywords</p>
<ul>
<li>“We hypothesize that…”（假设）</li>
<li>“We propose…”(建议)</li>
<li>“We introduce…”（提出）</li>
</ul>
</li>
<li><p>图表的标题</p>
</li>
<li><p>introduction的第一句和最后一句</p>
</li>
</ul>
<h2 id="notes"><a href="#notes" class="headerlink" title="notes"></a>notes</h2><p>做笔记也是阅读文献中比较重要的一步，用固定的格式比较方便查找：</p>
<ul>
<li><strong>Article title</strong>（文章标题）</li>
<li>作者，期刊</li>
<li>有关方向</li>
<li>阅读日期</li>
<li>网址</li>
<li>主要概念（推荐结构）</li>
<li>我自己的想法</li>
</ul>
]]></content>
      <tags>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>LPSNet</title>
    <url>/2024/03/16/LPSNet/</url>
    <content><![CDATA[<h1 id="Lightweight-and-Progressively-Scalable-Networks-for-Semantic-Segmentation"><a href="#Lightweight-and-Progressively-Scalable-Networks-for-Semantic-Segmentation" class="headerlink" title="Lightweight and Progressively-Scalable Networks for Semantic Segmentation"></a>Lightweight and Progressively-Scalable Networks for Semantic Segmentation</h1><h2 id="Abstract-摘要"><a href="#Abstract-摘要" class="headerlink" title="Abstract(摘要)"></a>Abstract(摘要)</h2><p>主要方法：通过一次性扩展单个维度（卷积块的数量、通道的数量或输入分辨率）来逐步将小型网络扩展到更大的网络，以满足最佳速度&#x2F;精度权衡。</p>
<h2 id="Conclusion-and-Discussion（总结和讨论）"><a href="#Conclusion-and-Discussion（总结和讨论）" class="headerlink" title="Conclusion and Discussion（总结和讨论）"></a>Conclusion and Discussion（总结和讨论）</h2><p>它是一种探索经济设计并逐步扩大网络以实现有效的语义分割</p>
<p>可能影响精度&#x2F;延迟的平衡的因素为：<strong>多路径框架中的基本卷积块和路径交互方式</strong></p>
<p>在卷积块中使用3×3Conv和双线性插值实现跨路径的交互</p>
<p>（上采样换成PointRend会怎样？）</p>
<p>先构建一个微型网络，然后一次性扩展单个维度将微型网络扩展为一系列较大的网络</p>
<h2 id="Introduction（引言）"><a href="#Introduction（引言）" class="headerlink" title="Introduction（引言）"></a>Introduction（引言）</h2><p>语义分割是为图像分割或视频帧的每个像素分配语义标签</p>
<p>多尺度学习沿三个不同维度进行语义分割：</p>
<ul>
<li>U-shape(结构分层融合特征，逐步提高空间分辨率)</li>
<li>pyramid pooling（在多个尺度上执行空间或空洞空间金字塔池化深入研究金字塔信息）</li>
<li>muti-path framework（将输入图像的大小调整为多个分辨率或尺度，并将每个尺度输入到深度学习的单独路径中）</li>
</ul>
<p>本文使用的就是muti-path framework，将输入分辨率从高到低并行放置，直接保持高分辨率信息，这样学习到的特征可能更有能力对每个像素进行分类和定位</p>
<ul>
<li>轻量化用于语义分割的计算单元</li>
<li>逐步扩大网络，同时平衡准确性和推理延迟</li>
</ul>
<p><strong>一次性扩展单个维度（卷积块的数量、通道的数量或输入分辨率）</strong></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/%E8%81%94%E6%83%B3%E6%88%AA%E5%9B%BE_20240315191556.png"
                      alt="联想截图_20240315191556"
                ></p>
<h3 id="contributions"><a href="#contributions" class="headerlink" title="contributions:"></a>contributions:</h3><p>(1) The lightweight design of convolutional blocks and the way of path interactions in multipath framework are shown capable of regarding as the practical principles for efficient semantic segmentation; （卷积块的轻量化设计和多路径框架中的路径交互方式实用）</p>
<p>(2) The exquisitely devised LPS-Net is shown able to progressively expand the network complexity while striking the right accuracy-efficiency tradeoff; （可扩展网络复杂性）</p>
<p>(3) LPSNet has been properly verified through extensive experiments over three datasets, and superior capability is observed on both NVIDIA GPUs and embedded devices in our experiments.（结果很不错）</p>
<h2 id="Results（结果）"><a href="#Results（结果）" class="headerlink" title="Results（结果）"></a>Results（结果）</h2><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20240315194455560.png"
                      alt="image-20240315194455560"
                ></p>
<h2 id="Methods（方法）"><a href="#Methods（方法）" class="headerlink" title="Methods（方法）"></a>Methods（方法）</h2><h3 id="Macro-Architecture（employs-the-multi-path-recipe）"><a href="#Macro-Architecture（employs-the-multi-path-recipe）" class="headerlink" title="Macro Architecture（employs the multi-path recipe）"></a>Macro Architecture（employs the multi-path recipe）</h3><p>LPS-Net 中采用多路径配方的宏架构</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20240315205546026.png"
                      alt="image-20240315205546026"
                ></p>
<p>（轻量化）三个设计原则：（卷积类型、通道数数量、跨多个路径的交互方式）</p>
<p>将输入图像的大小调整为多个尺度，并将每个尺度反馈送到单独的路径中。</p>
<p>将交互模块放置在阶段 3∼5 的末尾，旨在促进路径之间的相互交互。所有路径的输出被聚合并输入到分割头中，以生成具有 num 类通道的分数图。</p>
<p>对分数图执行双线性上采样，产生分辨率为 H×W 的输出，与输入分辨率完全匹配</p>
<h3 id="卷积块"><a href="#卷积块" class="headerlink" title="卷积块"></a>卷积块</h3><ul>
<li>卷积块的类型</li>
</ul>
<p>通过实验选择用标准卷积作为 <code>LPSNet</code> 中的构建块</p>
<ul>
<li>通道数</li>
</ul>
<p>具有 2n-divisible 通道宽度的卷积的可并行化实现。因此，将 <code>LPSNet</code> 中卷积的通道宽度设为2n-divisible，其中 <code>n</code> 尽可能大。</p>
<h3 id="Multi-Path交互"><a href="#Multi-Path交互" class="headerlink" title="Multi-Path交互"></a><strong>Multi-Path交互</strong></h3><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20240316154125443.png"
                      alt="image-20240316154125443"
                ></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20240316154135995.png"
                      alt="image-20240316154135995"
                ></p>
<h3 id="扩展算法"><a href="#扩展算法" class="headerlink" title="扩展算法"></a>扩展算法</h3><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20240316154602791.png"
                      alt="image-20240316154602791"
                ></p>
]]></content>
      <tags>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>deep Learning知识点</title>
    <url>/2024/04/24/%E7%BB%86%E7%A2%8E%E7%9F%A5%E8%AF%86%E7%82%B9/</url>
    <content><![CDATA[<h1 id="关于监督学习、无监督学习、半监督学习、强化学习、自监督学习的区别"><a href="#关于监督学习、无监督学习、半监督学习、强化学习、自监督学习的区别" class="headerlink" title="关于监督学习、无监督学习、半监督学习、强化学习、自监督学习的区别"></a>关于监督学习、无监督学习、半监督学习、强化学习、自监督学习的区别</h1><h3 id="监督学习（Supervised-Learning-或Supervised-Machine-Learning）"><a href="#监督学习（Supervised-Learning-或Supervised-Machine-Learning）" class="headerlink" title="监督学习（Supervised Learning 或Supervised Machine Learning）"></a>监督学习（Supervised Learning 或Supervised Machine Learning）</h3><p>使用标记数据集来训练算法，一边训练后的算法可以对数据进行分类或准确预测结果。</p>
<p>可分成两类：分类（线性分类器、支持向量机、决策树、随机森林等）、回归（使用一种算法理解因变量和自变量之间的关系，有助于根据不同的数据点来预测数值）</p>
<h3 id="无监督学习（Unsupervised-Learning）"><a href="#无监督学习（Unsupervised-Learning）" class="headerlink" title="无监督学习（Unsupervised Learning）"></a>无监督学习（Unsupervised Learning）</h3><p>用算法来分析并聚类未标记的数据集，以便发现数据中隐藏的模式和规律，而不需要人工干预。</p>
<p>主要用于三个任务：聚类、关联和降维</p>
<ul>
<li>聚类（Clustering）：数据挖掘技术，用于根据未标记数据的相似性或差异性对他们进行分类分组。适用于细分市场的划分、图像压缩等。</li>
<li>关联（Association）：使用不同的规则来查找给定数据集中变量之间的关系。常用于推荐算法。</li>
<li>降维（Dimensionality Reduction）:当特定数据集中的特征（或维度）太多时，在保持数据完整性的同时，将数据输入的数量（维度）减少到可管理可操作的大小。常用于数据预处理阶段，例如用自编码器把图片数据中的噪点去除，以提高图像质量。</li>
</ul>
<h4 id="对比："><a href="#对比：" class="headerlink" title="对比："></a>对比：</h4><p>监督学习和无监督学习本质区别就是<strong>用来训练的数据是否进行标注</strong>。</p>
<p>监督学习处理数据比较耗费算力，但结果比较准确，可以解释。无监督学习处理数据算力开销不大，但是无法解释，也许是可以挖掘出未被人类注意的新规律的。</p>
<h3 id="半监督学习（Semi-supervised-Learning）"><a href="#半监督学习（Semi-supervised-Learning）" class="headerlink" title="半监督学习（Semi-supervised Learning）"></a>半监督学习（Semi-supervised Learning）</h3><p>适用情况：相对较少的标记数据+大量未标记数据</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20240508170017070.png"
                      alt="image-20240508170017070"
                ></p>
<h3 id="强化学习（Reinforcement-Learning）"><a href="#强化学习（Reinforcement-Learning）" class="headerlink" title="强化学习（Reinforcement Learning）"></a>强化学习（Reinforcement Learning）</h3><p>对算法执行的正确和不正确行为分别进行奖励和惩罚的制度，目的是使算法获得最大的累积奖励，从而学会在特定环境下做出最佳决策。</p>
<ul>
<li>代理人，Agent：一个我们试图学习的实体（即玩家在游戏中所使用的角色）；</li>
<li>环境，Environment：代理人所处的环境（游戏所设置的游戏世界设定）；</li>
<li>状态，State：代理人在环境中获得自己当前状态的各种信息；</li>
<li>行动，Actions：代理人在环境中所执行的与环境交互的各种动作（马里奥游戏中的行走、跑步、跳跃等等）；</li>
<li>奖励，Reward：代理人从环境中获得的行动反馈（在马里奥的游戏里，即为正确的行动增加的积分&#x2F;硬币，是一个积极的奖励。因落入陷阱或被怪物吃掉而丢失积分，或损失一条“命”，则是一个消极的奖励）；</li>
<li>策略，Policy：根据代理人当前的状态决定一个合适的决策，以最大化地在未来某个时间段内获得正面报酬，最小化获得负面的惩罚；</li>
<li>价值函数， Value function：决定什么才是对代理人是有益的。</li>
</ul>
<h3 id="自监督学习（self-supervised-learning）SSL"><a href="#自监督学习（self-supervised-learning）SSL" class="headerlink" title="自监督学习（self-supervised learning）SSL"></a>自监督学习（self-supervised learning）SSL</h3><p>不需要人工标注训练数据，主要训练从大规模的无监督数据中挖掘能够应用于自身的监督信息，从而从输入的一部分数据中去学习另一部分。</p>
<p>自监督学习可以通过对图片的剪裁、九宫格切割后再打乱、镜像或降低色彩饱和度等操作，让机器学会改变后的图像与原图像之间存在着十分接近的联系，这种紧密联系在二维的 Embedding 坐标空间中显示为极度靠近的坐标点。不仅仅是图片，自监督学习可以对音频、视频、文本进行同样的学习。然而这些紧密的联系，是无法通过人类标注员来操作的。就好比我们可以对图中的鸟标注为“鸟”，但是自监督学习只会把它标注为 Embedding 空间中数据结构位置信息，这在本质上和人类给这幅图标注为“鸟”是一个意思。</p>
<p>可以看出，自监督学习很容易被误解为无监督学习中的聚类，因为他们也同样是<strong>把不同的未标记的事物进行分类</strong>，但其实自监督学习是在<strong>最大化同一类样本在 Embedding 空间中表征的相似性</strong>，同时<strong>最小化不同类样本之间表征的相似性</strong>。要做到相同类别的事物表达相近，不同类别的事物表达要更远，也就是说要极端化这种对比。通过这样的极端化过程，编码器（Encoder）能学到样本在 Embedding 空间中的许多潜在特征。所谓物以类聚，人以群分！</p>
<p>可以对巨量数据自动进行更广泛的标注，对下游任务产生帮助。也适合挖掘大量的数据集中不被人类关注过的“隐蔽”信息。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20240508172136916.png"
                      alt="image-20240508172136916"
                ></p>
<h2 id="分布式训练"><a href="#分布式训练" class="headerlink" title="分布式训练"></a>分布式训练</h2><p>由于硬件资源的限制，使用多台机器共同完成训练任务。</p>
<p>(以下是突然找到了好早之前的笔记)</p>
<h2 id="解耦设计"><a href="#解耦设计" class="headerlink" title="解耦设计"></a>解耦设计</h2><p>将不同部分分离开来，以提高灵活性、可维护性和性能。</p>
<p>这种方式减少了不同部分之间的依赖关系，使他们可以更加独立地设计、实现和维护。</p>
<h2 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h2><p>作用：有效的缓解过拟合现象</p>
<p>在batch中，忽略一半的特征检测器（让一半的隐层节点值为0）。这种方式减少了特征检测器（隐层节点）之间的相互作用</p>
<p>在前向传播的时候让某个神经元的激活值以一定的概率p停止工作，这样可以使模型的泛化性更强，因为它不会太依赖某些局部的特征</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20240116011233467.png"
                      alt="image-20240116011233467"
                ></p>
<p><strong>dropout程序导致两个神经元不一定每次都在一个dropout网络中出现。这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况 。迫使网络去学习更加鲁棒的特征</strong></p>
]]></content>
      <tags>
        <tag>notes</tag>
      </tags>
  </entry>
  <entry>
    <title>PointRend</title>
    <url>/2024/03/22/PointRend/</url>
    <content><![CDATA[<h1 id="PointRend-Image-Segmentation-as-Rendering"><a href="#PointRend-Image-Segmentation-as-Rendering" class="headerlink" title="PointRend: Image Segmentation as Rendering"></a><a class="link"   href="https://arxiv.org/abs/1912.08193" >PointRend: Image Segmentation as Rendering <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h1><p><strong>现有方法</strong>：一般会输出一个原图1&#x2F;8或1&#x2F;16大小的预测图，然后通过双线性插值来补齐最后的8x&#x2F;16x分辨率</p>
<p><strong>现有方法存在的问题</strong></p>
<p>过采样：对于图片中低频区域，没必要用太多的采样点，却使用太多采样点造成过采样；<br>欠采样 ：对于图片中高频区域（ 靠近物体边界 ），这些区域的采样过于稀疏，导致分割出的边界过于平滑。</p>
<p><strong>PointRend</strong></p>
<p>提出的 PointRend 是一个通用的模块，它允许许多可能的实现：输入一个或多个 feature map（通常比原图分辨率低 4x 或 16x），输出高分辨率的预测结果.</p>
<ul>
<li><p>PointRend 只对选择的点进行预测，而不是对所有输入点都进行预测.</p>
</li>
<li><p>对选择的点先通过插值的方式提取点特征，然后用 point head subnetwork 来预测点标签</p>
</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20240322121912139.png"
                      alt="image-20240322121912139"
                ></p>
<p><strong>Inference</strong></p>
<p>每个迭代循环中：<br>① 用双线性插值对上次预测出的 seg mask 进行上采样；<br>② 然后在上采样后的 mask 中找出 N 个最不确定的点（比如 binary mask 中 pribabilities 接近 0.5 的点）；<br>③ 用 pointrend 提取这些点的 feature 并预测它们的 label.</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20240322121856009.png"
                      alt="image-20240322121856009"
                ></p>
<p><strong>采样的三个原则</strong></p>
<p><strong>过生成</strong>: 从均匀分布中随机抽取 KN（K&gt;1）个点作为候选点。</p>
<p><strong>重要性抽样</strong>: 插值得到所有 KN 点处的的粗预测，然后计算这些点的不确定性，选择其中最不确定的 βN ( β∈[0,1] ) 个点。</p>
<p><strong>广域覆盖:</strong> 剩下的 (1−β)N 个点从均匀分布中随机采样得到</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20240322122545106.png"
                      alt="image-20240322122545106"
                ></p>
<p><strong>Coarse prediction features</strong></p>
<p><strong>必要性：</strong>只有点特征是不够的，因为 ① 对实例分割而言，一个点可能在两个实例的交叠处，需要区域信息来辅助判断；② 对实例分割和语义分割两项任务而言，点特征所使用的 feature map 可能只包含 low-level 的信息，因此还需要包含进去一些上下文和语义信息.</p>
<p>因此使用粗分割结果作为辅助信息，被涵盖在点特征表示向量中：一个点处表示 K 分类结果的 K 维向量.</p>
<p><strong>Point Head</strong></p>
<p>2 个 1024-wide hidden layers 的 MLP</p>
<p>类似于图卷积或 PointNet，这个 MLP 对所有的点<strong>共享参数.</strong></p>
]]></content>
      <tags>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>UPDP</title>
    <url>/2024/07/01/%E5%89%AA%E6%9E%9D/</url>
    <content><![CDATA[<h1 id="UPDP-A-Unified-Progressive-Depth-Pruner-for-CNN-and-Vision-Transformer"><a href="#UPDP-A-Unified-Progressive-Depth-Pruner-for-CNN-and-Vision-Transformer" class="headerlink" title="UPDP: A Unified Progressive Depth Pruner for CNN and Vision Transformer"></a>UPDP: A Unified Progressive Depth Pruner for CNN and Vision Transformer</h1><p>简要介绍：一种很厉害的剪枝方法，适用于cnn和transformer</p>
<h2 id="传统剪枝（pruning）"><a href="#传统剪枝（pruning）" class="headerlink" title="传统剪枝（pruning）"></a><a href="https://www.cnblogs.com/armcvai/p/17143077.html"><strong>传统剪枝</strong>（pruning）</a></h2><p>模型剪枝算是模型压缩的一种，直接减少参数量，为了减少对硬件的要求、加速模型推理和落地。（模型稀疏化）</p>
<p>做法：直接删除部分不重要的权重参数，减少参数量和计算量，尽量使精度不受影响。</p>
<p>在神经网络中，<strong>非结构化稀疏</strong>包括权重稀疏、激活稀疏、梯度稀疏</p>
<h4 id="权重稀疏"><a href="#权重稀疏" class="headerlink" title="权重稀疏"></a>权重稀疏</h4><p>权重的数值分布比较像正态分布，而且越接近0，权重越多。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20240624201620571.png"
                      alt="image-20240624201620571"
                ></p>
<p>卷积层的剪枝敏感性大于全连接层，且第一层卷积层最为敏感。</p>
<p>剪枝三段式工作 <code>pipeline</code> ：训练、剪枝、微调</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20240624201850731.png"
                      alt="image-20240624201850731"
                ></p>
<p>对硬件加速不友好，尤其是GPU，因为稀疏后得到的矩阵是高度非规则的矩阵。</p>
<h4 id="激活稀疏"><a href="#激活稀疏" class="headerlink" title="激活稀疏"></a>激活稀疏</h4><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20240624205508674.png"
                      alt="image-20240624205508674"
                ></p>
<p>APoZ高  –&gt;  冗余</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20240624205700813.png"
                      alt="image-20240624205700813"
                ></p>
<p>剪枝三段式工作：</p>
<ol>
<li>正常训练，然后在大型数据集上运行网络以获得每个神经元的APoZ。</li>
<li>根据标准修剪高APoZ的神经元，并移除相应的神经元连接。</li>
<li>使用修建前的权重初始化之后再重新训练一遍。</li>
</ol>
<h3 id="结构化稀疏（粗粒系稀疏、块稀疏）"><a href="#结构化稀疏（粗粒系稀疏、块稀疏）" class="headerlink" title="结构化稀疏（粗粒系稀疏、块稀疏）"></a>结构化稀疏（粗粒系稀疏、块稀疏）</h3><ul>
<li><code>channel/filter-wise</code> 减少网络通道</li>
<li><code>shape-wise</code></li>
</ul>
<p>一般都是丢弃整行或整列的权重，或者卷积层中的整个滤波器。</p>
<p>结构化剪枝包含通道剪枝和块剪枝等技术。通道修剪侧重于消除内核内的整个通道过滤器，而块修剪则在更大范围内进行，通常针对完整块。</p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>提出一种新颖的子网块修剪策略和渐进式训练方法，而且扩展到transformer模型，效果很好</p>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>主要贡献：</p>
<p>（1）我们提出了一种统一且高效的<strong>深度剪枝方法</strong>来优化 CNN 和视觉 Transformer 模型。</p>
<p> (2)我们提出了一种用于<strong>子网优化的渐进式训练策略</strong>，以及一种<strong>使用重新参数化技术的新颖的块修剪策略</strong>。 </p>
<p>(3) 对 CNN 和视觉 Transformer 模型进行全面的实验，以展示我们的深度剪枝方法的优越剪枝性能。</p>
<p>深度卷积减少计算量和参数，但是内存占用增加</p>
<h3 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h3><p>（未完待续）</p>
]]></content>
      <tags>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>python笔记</title>
    <url>/2024/10/18/python%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h2 id="运算符重载"><a href="#运算符重载" class="headerlink" title="运算符重载"></a>运算符重载</h2><p>python运算符用于内置类，但是相同的运算符对不同类型有不同的行为，python中的这一功能允许同一运算符根据上下文具有不同的语义，称为运算符过载。</p>
<h3 id="Python中的特殊函数"><a href="#Python中的特殊函数" class="headerlink" title="Python中的特殊函数"></a>Python中的特殊函数</h3><p>以双下划线__开头的类函数在Python中称为特殊函数。每次我们创建该类的新对象时都会调用它。Python中有很多特殊函数。</p>
<p>在类中定义 <code>__str()__</code>方法，可以控制他的打印输出方式。</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">class Point:</span><br><span class="line">    def __init__(self, x = 0, y = 0):</span><br><span class="line">        self.x = x</span><br><span class="line">        self.y = y</span><br><span class="line">    </span><br><span class="line">    def __str__(self):</span><br><span class="line">        return &quot;(&#123;0&#125;,&#123;1&#125;)&quot;.format(self.x,self.y)</span><br></pre></td></tr></table></figure></div>

<p>try：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; p1 = Point(2,3)</span><br><span class="line">&gt;&gt;&gt; print(p1)</span><br><span class="line">(2,3)</span><br></pre></td></tr></table></figure></div>

<h3 id="重载-（实现-add-函数）"><a href="#重载-（实现-add-函数）" class="headerlink" title="重载 +（实现 __add__()函数）"></a>重载 <code>+</code>（实现 <code>__add__()</code>函数）</h3><div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">class Point:</span><br><span class="line">    def __init__(self, x = 0, y = 0):</span><br><span class="line">        self.x = x</span><br><span class="line">        self.y = y</span><br><span class="line">    </span><br><span class="line">    def __str__(self):</span><br><span class="line">        return &quot;(&#123;0&#125;,&#123;1&#125;)&quot;.format(self.x,self.y)</span><br><span class="line">    </span><br><span class="line">    def __add__(self,other):</span><br><span class="line">        x = self.x + other.x</span><br><span class="line">        y = self.y + other.y</span><br><span class="line">        return Point(x,y)</span><br></pre></td></tr></table></figure></div>

<p>try:</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; p1 = Point(2,3)</span><br><span class="line">&gt;&gt;&gt; p2 = Point(-1,2)</span><br><span class="line">&gt;&gt;&gt; print(p1 + p2)</span><br><span class="line">(1,5)</span><br></pre></td></tr></table></figure></div>

<h3 id="python运算符重载的特殊函数"><a href="#python运算符重载的特殊函数" class="headerlink" title="python运算符重载的特殊函数"></a>python运算符重载的特殊函数</h3><table>
<thead>
<tr>
<th align="left">运算符</th>
<th align="left">表达</th>
<th align="left">在内部</th>
</tr>
</thead>
<tbody><tr>
<td align="left">相加（+）</td>
<td align="left">p1 + p2</td>
<td align="left">p1 .__ add __（p2）</td>
</tr>
<tr>
<td align="left">相减（-）</td>
<td align="left">p1-p2</td>
<td align="left">p1 .__ sub __（p2）</td>
</tr>
<tr>
<td align="left">相乘（*）</td>
<td align="left">p1 * p2</td>
<td align="left">p1 .__ mul __（p2）</td>
</tr>
<tr>
<td align="left">求幂（**）</td>
<td align="left">p1 ** p2</td>
<td align="left">p1 .__ pow __（p2）</td>
</tr>
<tr>
<td align="left">相除（&#x2F;）</td>
<td align="left">p1 &#x2F; p2</td>
<td align="left">p1 .__ truediv __（p2）</td>
</tr>
<tr>
<td align="left">整除（&#x2F;&#x2F;）</td>
<td align="left">p1 &#x2F;&#x2F; p2</td>
<td align="left">p1 .__ floordiv __（p2）</td>
</tr>
<tr>
<td align="left">求模 （%）</td>
<td align="left">p1％p2</td>
<td align="left">p1 .__ mod __（p2）</td>
</tr>
<tr>
<td align="left">按位左移（&lt;&lt;）</td>
<td align="left">p1 &lt;&lt; p2</td>
<td align="left">p1 .__ lshift __（p2）</td>
</tr>
<tr>
<td align="left">按位右移（&gt;&gt;）</td>
<td align="left">p1 &gt;&gt; p2</td>
<td align="left">p1 .__ rshift __（p2）</td>
</tr>
<tr>
<td align="left">按位与（and）</td>
<td align="left">p1 and p2</td>
<td align="left">p1 .__ and __（p2）</td>
</tr>
<tr>
<td align="left">按位或（or）</td>
<td align="left">p1 | 2</td>
<td align="left">p1 .__ or __（p2）</td>
</tr>
<tr>
<td align="left">按位异或（^）</td>
<td align="left">p1 ^ p2</td>
<td align="left">p1 .__ xor __（p2）</td>
</tr>
<tr>
<td align="left">按位否（~）</td>
<td align="left">〜p1</td>
<td align="left">p1 .__ invert __()</td>
</tr>
</tbody></table>
<h3 id="比较运算符重载"><a href="#比较运算符重载" class="headerlink" title="比较运算符重载"></a>比较运算符重载</h3><table>
<thead>
<tr>
<th align="left">操作符</th>
<th align="left">表达式</th>
<th align="left">内部</th>
</tr>
</thead>
<tbody><tr>
<td align="left">小于（&lt;）</td>
<td align="left">p1 &lt;p2</td>
<td align="left">p1 .__ lt __（p2）</td>
</tr>
<tr>
<td align="left">小于等于（&lt;&#x3D;）</td>
<td align="left">p1 &lt;&#x3D; p2</td>
<td align="left">p1 .__ le __（p2）</td>
</tr>
<tr>
<td align="left">等于（&#x3D;&#x3D;）</td>
<td align="left">p1 &#x3D;&#x3D; p2</td>
<td align="left">p1 .__ eq __（p2）</td>
</tr>
<tr>
<td align="left">不等于（!&#x3D;）</td>
<td align="left">p1！&#x3D; p2</td>
<td align="left">p1 .__ ne __（p2）</td>
</tr>
<tr>
<td align="left">大于（&gt;）</td>
<td align="left">p1&gt; p2</td>
<td align="left">p1 .__ gt __（p2）</td>
</tr>
<tr>
<td align="left">大于等于（&gt;&#x3D;）</td>
<td align="left">p1&gt; &#x3D; p2</td>
<td align="left">p1 .__ ge __（p2）</td>
</tr>
</tbody></table>
]]></content>
  </entry>
  <entry>
    <title>SAM</title>
    <url>/2024/08/20/SAM/</url>
    <content><![CDATA[<h1 id="Segment-Anything"><a href="#Segment-Anything" class="headerlink" title="Segment Anything"></a>Segment Anything</h1><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20240922195105165.png"
                      alt="image-20240922195105165"
                ></p>
<h2 id="Task（提示分割任务）"><a href="#Task（提示分割任务）" class="headerlink" title="Task（提示分割任务）"></a>Task（提示分割任务）</h2><p>作为预训练，流程就是：给一个prompt（提示）和对应的图，返回一个有效的掩码。</p>
<p>仔细解释一下，</p>
<ul>
<li><p>prompt只是指定在图像中分割什么。</p>
</li>
<li><p>有效输出掩码意味着，即使给的提示不明确甚至可能涉及多个对象，输出也应该是一个合理的掩码的至少其中一个对象。</p>
</li>
</ul>
<h2 id="Model（SAM）"><a href="#Model（SAM）" class="headerlink" title="Model（SAM）"></a>Model（SAM）</h2><p>首先要满足三个条件：</p>
<ul>
<li>必须支持灵活的提示</li>
<li>需要实时计算掩码以允许交互使用</li>
<li>必须具有模糊意识</li>
</ul>
<p>流程如下：prompt进入提示编码器嵌入提示，image进入图像编码器计算图像嵌入，然后将两个信息源组合在一个轻量级掩码解码器中（在解码器里预测分割mask），然后输出一个有效的掩码。</p>
<p>在这其中，可以使用不同的prompt搭配相同的图像嵌入。</p>
<p>为了SAM能够更好的感知处理歧义，作者还预测单个提示的多个掩码。</p>
<p>细节：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20240922213002034.png"
                      alt="image-20240922213002034"
                ></p>
<p><code>Image encoder</code>(图像编码器)：使用MAE预训练的vit，为了处理高分辨率输入。</p>
<p><code>Prompt encoder</code>(提示编码器)：使用CLIP中现成的文本编码器对每种提示类型和自由格式文本进行学习嵌入。使用卷积嵌入mask，然后与图像嵌入按元素求和（add）。</p>
<p><code>Mask decoder</code>(掩码编码器)：掩码解码器有效地将图像嵌入、提示嵌入和输出标记映射到掩码。</p>
<p>过程：采用了 Transformer 解码器块 的修改，后跟动态掩模预测头。修改后的解码器块使用两个方向的即时自注意力和交叉注意力（即时图像嵌入，反之亦然）来更新所有嵌入。运行两个块后，我们对图像嵌入进行上采样，并且 MLP 将输出标记映射到动态线性分类器，然后计算每个图像位置的掩模前景概率。</p>
<p><code>Resolving ambiguity</code>（解决歧义）：改成预测单个提示的多个输出掩码，发现三个掩码输出足以解决最常见的问题（整体、部分、子部分）</p>
<p><code>Losses</code>：我们使用焦点损失和骰子损失的线性组合来监督掩模预测。</p>
<h2 id="Data（Data-engine数据引擎-Dataset）"><a href="#Data（Data-engine数据引擎-Dataset）" class="headerlink" title="Data（Data engine数据引擎 &amp; Dataset）"></a>Data（Data engine数据引擎 &amp; Dataset）</h2><p>为了构建了一个数据引擎，分为三个阶段：辅助手动、半自动和全自动。</p>
<p>在第一阶段，SAM 协助注释者注释掩模，类似于经典的交互式分割设置。</p>
<p>在第二阶段，SAM 可以通过提示可能的对象位置来自动为对象子集生成掩码，而注释器则专注于注释其余对象，从而帮助增加掩码多样性。</p>
<p>在最后阶段，我们使用前景点的规则网格提示 SAM，每张图像平均产生 大概100 个高质量掩模。</p>
<p>数据集是SA-1B，包括来自 11M 许可和隐私保护图像的超过 1B 个掩码。</p>
]]></content>
      <tags>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>RTSeg</title>
    <url>/2023/11/16/RTSeg/</url>
    <content><![CDATA[<h2 id="RTSeg-Real-time-Semantic-Segmentation-Comparative-Study"><a href="#RTSeg-Real-time-Semantic-Segmentation-Comparative-Study" class="headerlink" title="RTSeg: Real-time Semantic Segmentation Comparative Study"></a><a class="link"   href="https://arxiv.org/pdf/1803.02758v5.pdf" >RTSeg: Real-time Semantic Segmentation Comparative Study <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h2><p>这一篇更加关注计算效率，以下是一些些翻译加一些些理解。</p>
<p>针对于编码和解码模块设计出了可以灵活替换的子模块，方便大家可以方便的替换编码或者解码模块，从而针对不同任务设计不同的网络结构。</p>
<h2 id="ABSTRACT（摘要）"><a href="#ABSTRACT（摘要）" class="headerlink" title="ABSTRACT（摘要）"></a>ABSTRACT（摘要）</h2><p>Semantic segmentation benefits robotics related applications, especially autonomous driving. Most of the research on semantic segmentation only focuses on increasing the accuracy of segmentation models with little attention to computationally efficient solutions. The few work conducted in this direction does not provide principled methods to evaluate the different design choices for segmentation. In this paper, we address this gap by presenting a real-time semantic segmentation benchmarking framework with a decoupled design for feature extraction and decoding methods. The framework is comprised of different network architectures for feature extraction such as VGG16, Resnet18, MobileNet, and ShuffleNet. It is also comprised of multiple meta-architectures for segmentation that define the decoding methodology. These include SkipNet, UNet, and Dilation Frontend. Experimental results are presented on the Cityscapes dataset for urban scenes. The modular design allows novel architectures to emerge, that lead to 143x GFLOPs reduction in comparison to SegNet. This benchmarking framework is publicly available at 1 .</p>
<p>在语义分割上大多数都是提升精度，但是很少关注计算效率高的解决方案，针对这一空白，提出了一个实时语义分割基准框架，对特征提取和解码进行了解耦设计。</p>
<p>特征提取用了不同的网络结构：VGG16、Resnet18、MobileNet和ShuffleNet；解码是由多个用于分割的元架构定义的：SkipNet、UNet和Dilation Frontend。</p>
<h2 id="INTRODUCTION（介绍）"><a href="#INTRODUCTION（介绍）" class="headerlink" title="INTRODUCTION（介绍）"></a>INTRODUCTION（介绍）</h2><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20231101224230740.png"
                      alt="image-20231101224230740"
                ></p>
<p>主要贡献：</p>
<ol>
<li>将特征提取模块和解码器进行了模块化解耦，并将器成为元架构（有助于理解网络不同部分对实时性能的影响）</li>
<li>消融实验突出了精度和速度的平衡</li>
<li>我们框架的模块化设计出现了两种新颖的分割架构，分别使用MobileNet [14] 和具有多种解码方法的 ShuffleNet [15]。与 SegNet 相比，ShuffleNet 减少了 143 倍的 GFLOPs。</li>
</ol>
<h3 id="SkipNet"><a href="#SkipNet" class="headerlink" title="SkipNet"></a><a class="link"   href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Xin_Wang_SkipNet_Learning_Dynamic_ECCV_2018_paper.pdf" >SkipNet <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><h3 id="解码器模块介绍"><a href="#解码器模块介绍" class="headerlink" title="解码器模块介绍"></a>解码器模块介绍</h3><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/hello.png"
                      alt="屏幕截图 2023-11-14 172740"
                ></p>
<p> 图2（a）是SkipNet的解码结构，类似于FCN8s的结构，其中较高高分辨率的特征图通过1x1卷积来将通道数量减少到最终的类别数量，每一个通道都对应着一个类别。</p>
<p>图2（b）是UNet的解码结构，Unet结构提供的解码方式为：利用反卷积，将与下采样阶段对应的特征图进行上采样。上采样的特征图与下采样中有相同分辨率的特征图进行融合。逐级向上采样提供的精度比一次8倍向上采样的精度更高。目前采用的融合方法是逐元素相加，concatenation的方法可以提供更高的准确率，因为其确保了网络能够学习特征的加权融合，但是这样会增加计算量(concatenation会改变通道数量)。上采样之后的特征最后会接一个1x1的卷积来输出最后的逐元素分类。</p>
<p>对于Dilation Frontend的解码结构，文中并没有给出示意图，Dilation Frontend结构利用了空洞卷积来取代下采样。空洞卷积确保了网络能够保留足够的感受野的同时，不会降低特征图的分辨率。但是副作用就是计算量的增加，修改编码器网络使得下采样率从32变为8。下采样的减少是通过删除池化层或将步幅为2的卷积转换为步幅为1的卷积来完成的。然后，将池化或正常的卷积替换为两个空洞率为2和4的空洞卷积[3]。</p>
]]></content>
      <tags>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>SAM 2</title>
    <url>/2024/08/22/SAM2/</url>
    <content><![CDATA[<h1 id="Segment-Anything-in-Images-and-Videos"><a href="#Segment-Anything-in-Images-and-Videos" class="headerlink" title="Segment Anything in Images and Videos"></a>Segment Anything in Images and Videos</h1><h2 id="Introduction-介绍"><a href="#Introduction-介绍" class="headerlink" title="Introduction(介绍)"></a>Introduction(介绍)</h2><p>比起SAM，SAM2可以看作是SAM的拓展，从静态的图像分割，到动态的视频分割。准确度和速度都提升了。</p>
<p>（未完待续）</p>
]]></content>
      <tags>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>SkipNet</title>
    <url>/2023/11/16/SkipNet/</url>
    <content><![CDATA[<h1 id="SkipNet"><a href="#SkipNet" class="headerlink" title="SkipNet"></a><a class="link"   href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Xin_Wang_SkipNet_Learning_Dynamic_ECCV_2018_paper.pdf" >SkipNet <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h1><p>代码：<a class="link"   href="https://github.com/ucbdrive/skipnet" >https://github.com/ucbdrive/skipnet <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<h2 id="Abstract-（摘要）"><a href="#Abstract-（摘要）" class="headerlink" title="Abstract. （摘要）"></a>Abstract. （摘要）</h2><p>While deeper convolutional networks are needed to achieve maximum accuracy in visual perception tasks, for many inputs shallower networks are sufficient. We exploit this observation by learning to skip convolutional layers on a per-input basis. We introduce SkipNet, a modified residual network, that uses a gating network to selectively skip convolutional blocks based on the activations of the previous layer. We formulate the dynamic skipping problem in the context of sequential decision making and propose a hybrid learning algorithm that combines supervised learning and reinforcement learning to address the challenges of non-differentiable skipping decisions. We show SkipNet reduces computation by 30 − 90% while preserving the accuracy of the original model on four benchmark datasets and outperforms the state-of-the-art dynamic networks and static compression methods. We also qualitatively evaluate the gating policy to reveal a relationship between image scale and saliency and the number of layers skipped.</p>
<p>尽管需要更深的卷积网络才能在视觉感知任务中获得更大的准确性，但对于大多数输入来说，较浅的网络就够了。</p>
<p>SkipNet是经过改进的残差网络，使用gating network根据前一层的激活有选择地跳过卷积层。我们在顺序决策（sequential decision）的背景下制定动态跳过（dynamic skipping）问题，并提出一种混合学习算法，将监督学习和强化学习相结合，以解决不可微分的跳过动态的问题。</p>
<p>计算量减少了30%至90%，同时在四个基准数据集上保留了原始模型的准确性，并且胜过了最新的动态网络和静态压缩方法。</p>
<p>定性评估gating策略，以揭示图像比例和显着性（saliency）与跳过的层数之间的关系。</p>
<h2 id="Introduction（介绍）"><a href="#Introduction（介绍）" class="headerlink" title="Introduction（介绍）"></a>Introduction（介绍）</h2><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20231116225046629.png"
                      alt="image-20231116225046629"
                ></p>
<p>动态选择在推理过程中应跳过卷积神经网络的哪些层，动态跳过问题-构造-&gt;顺序决策问题，其中前一层的输出决定是否绕过后一层。</p>
<p>动态跳过问题的目标是保留整个网络准确性的同时，跳过尽可能多的层。（减少参数量，观察每层的作用）</p>
<p>为了在保持准确性的同时减少计算量，我们需要正确绕过网络中不必要的层，学习有效的跳过策略也是一项挑战。</p>
<p>（同时因为会跳过层数，所以不能应用梯度的优化）</p>
<p>过程：将gating module明确分配给每组卷积层（each group of layers）。gating模块将前一层的激活映射到二进制决策，以跳过或执行后一层。我们分两个阶段训练门控模块。首先，我们通过采用重新参数化(reparametrization)技巧对二进制<a class="link"   href="https://so.csdn.net/so/search?q=skip&spm=1001.2101.3001.7020" >skip <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>决策使用soft-max松弛，并结合原始模型使用的标准交叉熵损失一起训练layers and gates。然后，我们将概率gate输出视为初始跳过策略，并在不使用relaxation的情况下使用REINFORCE来优化策略。在后期阶段，我们共同优化跳过策略和预测误差，以稳定探索过程。</p>
<h2 id="Related-Work（相关工作）"><a href="#Related-Work（相关工作）" class="headerlink" title="Related Work（相关工作）"></a>Related Work（相关工作）</h2><p>加速现有的卷积网络：（训练初始网络后使用）权重稀疏化、滤波器修剪、矢量量化和蒸馏-&gt;将模型转移到较浅的网络上进行模型压缩</p>
<p>他人实验探索：通过提早终止来动态缩放计算、暂停循环网络以节省计算成本、在卷积网络中使用尽早终止、ResNets每组块中的提前终止</p>
<p>SkipNet并不会提早退出，而是根据处理层的输出有条件地绕过各个层，可以更好地权衡成本</p>
<h2 id="SkipNet-Model-Design（模型构思）"><a href="#SkipNet-Model-Design（模型构思）" class="headerlink" title="SkipNet Model Design（模型构思）"></a>SkipNet Model Design（模型构思）</h2><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20231116231511966.png"
                      alt="image-20231116231511966"
                ></p>
<p>对于给定输入有选择地包括或排除了各个层。 使用插入在各层之间的小型门控网络可以完成各层的按输入选择。 gating网络将前一层或一组层的输出映射到一个二进制决策，以执行或绕过后一层或一组层</p>
<p>对于a，当gating module独立的时候，情况变复杂的时候参数量变多</p>
<h3 id="Gating-Network-Design"><a href="#Gating-Network-Design" class="headerlink" title="Gating Network Design"></a>Gating Network Design</h3>]]></content>
      <tags>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>WTConv</title>
    <url>/2024/10/12/WTConv/</url>
    <content><![CDATA[<h2 id="Abstract-摘要"><a href="#Abstract-摘要" class="headerlink" title="Abstract(摘要)"></a>Abstract(摘要)</h2><p>新提出的WTConv（小波卷积）可以获得非常大的感受野而不会受到过度参数化的影响。</p>
<h2 id="Discussion（讨论）"><a href="#Discussion（讨论）" class="headerlink" title="Discussion（讨论）"></a>Discussion（讨论）</h2><p>使用WTConv，我们可以以纯卷积方法配置全局感受野的空间混合。</p>
<p>WTConv 大大增加了 CNN 的有效感受野，改善了 CNN 的形状偏差，使网络对损坏更加鲁棒，并为各种视觉任务提供了更好的性能。</p>
<h3 id="Limitations-限制"><a href="#Limitations-限制" class="headerlink" title="Limitations(限制)"></a>Limitations(限制)</h3><p>运行时间在现有框架内相对较高，这是由于多个顺序操作（WT-conv-IWT）的开销造成的，这可能比本身的成本更高。</p>
<p>可以通过使用专门的实现来缓解，例如，在每个级别中与卷积并行执行 WT 以减少内存读取，或就地执行 WT 和 IWT 以减少内存分配。</p>
<h2 id="Introduction（导言）"><a href="#Introduction（导言）" class="headerlink" title="Introduction（导言）"></a>Introduction（导言）</h2><p>一开始，大家认为vit的多头自注意力层有利于特征的全局混合，而卷积在结构上仅限于特征的局部混合。（性能差距）为了弥补性能差距，把卷积核增大到7×7大小，但是内核会变得过度参数化，并且在达到全局感受野之前性能会饱和。</p>
<p>发现，使用较大的内核使 CNN 的形状偏差更大，这意味着它们捕获图像中低频的能力得到了提高。但是卷积层通常倾向于响应输入中的高频。而注意力头更适合低频。</p>
<p>所以</p>
<p><strong>idea</strong>：利用时频分析中的小波变换（WT），使卷积的感受野能够很好地放大，并通过级联引导 CNN 更好地响应低频。</p>
<p><strong>movivation</strong>：保留了一定的空间分辨率，这使得空间运算（比如卷积）更加有意义。</p>
<p>提出了 WTConv，该层使用级联 WT 分解并执行一组小核卷积，每个卷积都专注于越来越大的感受野中输入的不同频段。这个过程使我们能够更加重视输入中的低频，同时仅添加少量可训练参数。（感受野大，参数量少）</p>
<p>对于k×k感受野，参数量随k呈对数增长。</p>
<p>To summarize, our <strong>key contributions</strong> are:（主要贡献）</p>
<p> – A new layer, called WTConv, that uses the WT to increase the receptive field of convolutions effectively. （有效地增加卷积的感受野）</p>
<p>– WTConv is designed to be a drop-in replacement (for depth-wise convolutions) within given CNNs. （WTConv可替代CNN的Conv）</p>
<p>– Extensive empirical evaluation demonstrates that WTConv improves CNNs’ results in several key computer-vision tasks. （改善cv任务的结果）</p>
<p>– Analysis of WTConv’s contribution to CNN’s scalability, robustness, shapebias, and ERF.（可扩展性、鲁棒性）</p>
<h2 id="Methods（方法）"><a href="#Methods（方法）" class="headerlink" title="Methods（方法）"></a>Methods（方法）</h2><h3 id="Preliminaries-The-Wavelet-Transform-as-Convolutions-小波变换"><a href="#Preliminaries-The-Wavelet-Transform-as-Convolutions-小波变换" class="headerlink" title="Preliminaries: The Wavelet Transform as Convolutions(小波变换)"></a>Preliminaries: The Wavelet Transform as Convolutions(小波变换)</h3><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20241012113051486.png"
                      alt="image-20241012113051486"
                ></p>
<p>我们在两个维度上操作，使用以下四个滤波器组产生步幅为 2 的深度卷积：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20241012113111025.png"
                      alt="image-20241012113111025"
                ></p>
<p>fLL为低通滤波器，其余三个为高通滤波器，对于每个输入通道，卷积的输出：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20241012113120298.png"
                      alt="image-20241012113120298"
                ></p>
<p>XLL 是 X 的低频分量，而 XLH 、 XHL 、 XHH 是其水平、垂直和对角高频分量。由于公式（1）中形成了正交基，所以应用小波逆变换（IWT）通过转置卷积得到X：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20241012113128778.png"
                      alt="image-20241012113128778"
                ></p>
<p>然后通过递归分解低频分量给出级联小波分解。每个分解级别由下式给出：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20241012113138305.png"
                      alt="image-20241012113138305"
                ></p>
<p>由迭代分解低频分量可以分析出，小波变换更加关注于低频，较低频率的频率分辨率增加，空间分辨率降低。输入低频的重复小波变换分解强调了它们并增加了层的相应响应。</p>
<h3 id="Convolution-in-the-Wavelet-Domain（小波域中的卷积）"><a href="#Convolution-in-the-Wavelet-Domain（小波域中的卷积）" class="headerlink" title="Convolution in the Wavelet Domain（小波域中的卷积）"></a>Convolution in the Wavelet Domain（小波域中的卷积）</h3><p>已知，增加卷积核的大小时参数量以2次方增长，为了缓解这种情况，使用小波变换来过滤和缩小输入的低频和高频内容。然后，在使用 IWT 构造输出之前，对不同频率图执行小内核深度卷积。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20241012114015929.png"
                      alt="image-20241012114015929"
                ></p>
<p>X 是输入张量，W 是 k × k 深度内核的权重张量，其输入通道数是 X 的四倍。（刚好和分离出来的不同频率分量的卷积相匹配）该操作分离了频率分量之间的卷积，还允许较小的内核在原始输入的较大区域中进行操作。</p>
<p>采用该 1 级组合运算，并使用等式 1 中相同的级联原理进一步增加等式4，该过程由下式给出：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20241012114751984.png"
                      alt="image-20241012114751984"
                ></p>
<p> X(0) LL 是层的输入，X(i) H 表示级别 i 的所有三个高频图X(i)LH,X(i)HL,X(i)HH。</p>
<p>为了组合不同频率的输出, WT 及其逆是线性运算,意味着 IWT(X + Y ) &#x3D; IWT(X) + IWT(Y )，所以得到：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20241012115300453.png"
                      alt="image-20241012115300453"
                ></p>
<p> Z(i) 是从级别 i 开始的聚合输出。不同大小卷积的两个输出相加作为输出。</p>
<p>例：使用 2 级小波分解和 3 × 3 卷积核大小。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20241012115518981.png"
                      alt="image-20241012115518981"
                ></p>
<p>不能对 Y (i) LL 、 Y (i) H 中的每一个进行归一化，因为它们的单独归一化并不对应于原始域中的归一化。仅执行通道缩放来权衡每个频率分量足够。</p>
<h3 id="The-Benefits-of-Using-WTConv（使用WTConv的好处）"><a href="#The-Benefits-of-Using-WTConv（使用WTConv的好处）" class="headerlink" title="The Benefits of Using WTConv（使用WTConv的好处）"></a>The Benefits of Using WTConv（使用WTConv的好处）</h3><p>将 WTConv 合并到给定的 CNN 中有两个主要的技术优势：</p>
<ul>
<li>WT 的 l 级级联频率分解，与每个级别的固定大小内核 k 一起，允许参数数量在级别数中线性缩放而感受野呈指数增长。</li>
<li>WTConv 层的构建比标准卷积更好地捕获低频。因为输入低频的重复小波变换分解强调了它们并增加了层的相应响应。</li>
</ul>
<h3 id="Computational-Cost（计算成本）"><a href="#Computational-Cost（计算成本）" class="headerlink" title="Computational Cost（计算成本）"></a>Computational Cost（计算成本）</h3><p>深度卷积的计算成本为</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20241012120342993.png"
                      alt="image-20241012120342993"
                ></p>
<p>其中 C 是输入通道数，(NW , NH ) 是输入的空间维度，(KW , KH ) 是内核大小，(SW , SH ) 是每个维度的步幅。</p>
<p>通过使用标准卷积运算的简单实现，WT 的 FLOP 计数为</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20241012120843103.png"
                      alt="image-20241012120843103"
                ></p>
<p>因为四个内核的大小为 2×2，每个空间维度上的步幅为 2，并且在每个输入通道上运行。</p>
<p>比较起来，这仍然比类似感受野的标准深度卷积节省了很多。</p>
<h2 id="Results（结果）"><a href="#Results（结果）" class="headerlink" title="Results（结果）"></a>Results（结果）</h2><p> 形状偏差</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20241012121934132.png"
                      alt="image-20241012121934132"
                ></p>
<p>形状偏差与人类的感知有关，被认为是比较理想的。</p>
<p>消融实验</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20241012121859356.png"
                      alt="image-20241012121859356"
                ></p>
]]></content>
      <tags>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>SQL注入</title>
    <url>/2024/11/02/SQL%E6%B3%A8%E5%85%A5/</url>
    <content><![CDATA[<h2 id="SQL注入分类"><a href="#SQL注入分类" class="headerlink" title="SQL注入分类"></a>SQL注入分类</h2><p>SQL注入按照类型分为数字型注入和字符型注入。注入点的数据类型为数字型时为数字型注入，注入点的数据类型为字符型为字符型注入。<br> SQL注入按照服务器返回信息是否显示分为报错注入和盲注。如果在注入的过程中，程序将获取的信息或者报错信息直接显示在页面中，这样的注入为报错注入；如果在注入的过程中，程序不显示任何SQL报错信息，只能通过精心构造SQL语句，根据页面是否正常返回或者返回的时间判断注入的结果，这样的注入为盲注。</p>
<h2 id="数字型注入"><a href="#数字型注入" class="headerlink" title="数字型注入"></a>数字型注入</h2><p>数字型注入就是注入点的数据类型是数字型，没有用单引号引起来。数字型注入的典型示例代码</p>
<div class="highlight-container" data-rel="Php"><figure class="iseeu highlight php"><table><tr><td class="code"><pre><span class="line"><span class="variable">$id</span> = <span class="variable">$_GET</span>[<span class="string">&#x27;id&#x27;</span>];</span><br><span class="line"><span class="variable">$sql</span> = <span class="string">&quot;SELECT * FROM users WHERE id=<span class="subst">$id</span> LIMIT 0,1&quot;</span>;</span><br><span class="line"><span class="variable">$result</span> = <span class="title function_ invoke__">mysql_query</span>(<span class="variable">$sql</span>);</span><br><span class="line"><span class="variable">$row</span> = <span class="title function_ invoke__">mysql_fetch_array</span>(<span class="variable">$result</span>)</span><br></pre></td></tr></table></figure></div>

<p><strong>判断数字型注入的方法如下</strong><br> <strong>1. 输入单引号，不正常返回</strong><br> 如果用户提交index.php?id&#x3D;1’，那么后面的SQL语句就变成为SELECT * FROM users WHERE id&#x3D;1’ LIMIT0,1,SQL语句本身存在语法错误，会有不正常的结果返回。<br> <strong>2. 输入and 1&#x3D;1，正常返回</strong><br> 如果用户提交index.php?id&#x3D;1 and 1&#x3D;1，那么后面的SQL语句就变成为SELECT * FROM users WHERE id&#x3D;1 and 1&#x3D;1 LIMIT 0,1，会有正常的结果返回。<br> <strong>3. 输入and 1&#x3D;2，不正常返回</strong><br> SQL语句变为SELECT * FROM users WHERE id&#x3D;1 and 1&#x3D;2 LIMIT 0,1,会有不正常的结果返回。</p>
<h2 id="字符型注入"><a href="#字符型注入" class="headerlink" title="字符型注入"></a>字符型注入</h2><p>字符型注入就是注入点的数据类型是字符型。字符型注入与数字型注入的区别就是字符型注入要用一对单引号引起来。字符型注入的典型示例代码如下：</p>
<div class="highlight-container" data-rel="Php"><figure class="iseeu highlight php"><table><tr><td class="code"><pre><span class="line"><span class="variable">$id</span> = <span class="variable">$_GET</span>[<span class="string">&#x27;id&#x27;</span>];</span><br><span class="line"><span class="variable">$sql</span> = <span class="string">&quot;select * from users where id=&#x27;<span class="subst">$id</span>&#x27; limit 0,1;</span></span><br><span class="line"><span class="string"><span class="subst">$result</span> = mysql_query(<span class="subst">$sql</span>);</span></span><br><span class="line"><span class="string"><span class="subst">$row</span> = mysql_fetch_array(<span class="subst">$result</span>);</span></span><br></pre></td></tr></table></figure></div>

<p><strong>判断字符型注入的方法如下</strong><br> <strong>1. 输入单引号，不正常返回</strong><br> 输入单引号后，SQL语句变为select * from users where id&#x3D;1’ limit 0,1，SQL语句本身存在语法错误，会有不正常的结果返回<br> <strong>2. 输入’ and ‘1’&#x3D;’1，正常返回</strong><br> SQL语句变为select * from users where id&#x3D;’1’ and ‘1’&#x3D;’1’ limit 0,1，会有正常结果返回。<br> <strong>3. 输入’ and ‘1’&#x3D;’2，会不正常返回</strong><br> SQL语句变为select * from users where id&#x3D;’1’ and ‘1’&#x3D;’2’ limit 0,1，会有不正常的结果返回</p>
]]></content>
  </entry>
  <entry>
    <title>FLANet</title>
    <url>/2024/11/11/FLANet/</url>
    <content><![CDATA[<p>全称:Fully Attentional Network for Semantic Segmentation</p>
<p>全注意力模块代码:</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FullyAttentionalBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, plane, norm_layer=SyncBatchNorm</span>):</span><br><span class="line">        <span class="built_in">super</span>(FullyAttentionalBlock, self).__init__()</span><br><span class="line">        <span class="comment">#用于在高度和宽度方向上编码特征的线性层</span></span><br><span class="line">        self.conv1 = nn.Linear(plane, plane)</span><br><span class="line">        self.conv2 = nn.Linear(plane, plane)</span><br><span class="line">        </span><br><span class="line">        self.conv = nn.Sequential(nn.Conv2d(plane, plane, <span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">                                  norm_layer(plane),</span><br><span class="line">                                  nn.ReLU())</span><br><span class="line"></span><br><span class="line">        self.softmax = nn.Softmax(dim=-<span class="number">1</span>)</span><br><span class="line">        self.gamma = nn.Parameter(torch.zeros(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        batch_size, _, height, width = x.size()</span><br><span class="line"></span><br><span class="line">        feat_h = x.permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>).contiguous().view(batch_size * width, -<span class="number">1</span>, height)</span><br><span class="line">        feat_w = x.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>).contiguous().view(batch_size * height, -<span class="number">1</span>, width)</span><br><span class="line">        encode_h = self.conv1(F.avg_pool2d(x, [<span class="number">1</span>, width]).view(batch_size, -<span class="number">1</span>, height).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>).contiguous())</span><br><span class="line">        encode_w = self.conv2(F.avg_pool2d(x, [height, <span class="number">1</span>]).view(batch_size, -<span class="number">1</span>, width).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>).contiguous())</span><br><span class="line"></span><br><span class="line">        energy_h = torch.matmul(feat_h, encode_h.repeat(width, <span class="number">1</span>, <span class="number">1</span>))<span class="comment">#通过矩阵乘法计算高度方向的能量分数。</span></span><br><span class="line">        energy_w = torch.matmul(feat_w, encode_w.repeat(height, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        full_relation_h = self.softmax(energy_h)  <span class="comment"># [b*w, c, c]   获得高度方向的注意力权重</span></span><br><span class="line">        full_relation_w = self.softmax(energy_w)</span><br><span class="line"></span><br><span class="line">        full_aug_h = torch.bmm(full_relation_h, feat_h).view(batch_size, width, -<span class="number">1</span>, height).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>)<span class="comment">#使用批量矩阵乘法 (torch.bmm) 将高度方向的注意力应用于 feat_h</span></span><br><span class="line">        full_aug_w = torch.bmm(full_relation_w, feat_w).view(batch_size, height, -<span class="number">1</span>, width).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">        out = self.gamma * (full_aug_h + full_aug_w) + x </span><br><span class="line">        out = self.conv(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure></div>

<h2 id="abstract"><a href="#abstract" class="headerlink" title="abstract"></a>abstract</h2><p>在原来可以通过压缩空间维度或通过压缩通道的相似图来描述沿通道或空间维度的特征关系，但是这样会沿着其他维度压缩特征依赖性，就会导致注意力缺失，对小类别的结果较差或者大对象内分割不一致。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>在FLANet中，将空间和通道注意力编码到在单个相似图中，同时保持高计算效率。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20241111113854239.png"
                      alt="image-20241111113854239"
                ></p>
<p>空间NL可以增强细节的辨别力，而通道NL则有利于保持大对象内部的语义一致性空间NL可以增强细节的辨别力，而通道NL则有利于保持大对象内部的语义一致性。</p>
<p>而且单纯把两个NL模块堆叠起来使用还是会出现注意力缺失的问题，注意力缺失问题会损害特征表示能力，并且不能通过简单地堆叠不同的NL块来解决。，但是FLA解决了这一问题。</p>
<p>FLA基本思想:在计算通道注意力图时利用全局上下文信息来接收空间响应，从而能够在单个注意力单元中实现充分的注意力，并且具有较高的计算效率。具体来说，我们首先使每个空间位置能够从具有相同水平和垂直坐标的全局上下文中获取特征响应。其次，我们使用自注意力机制来捕获任意两个通道图之间的完全注意力相似性以及相关的空间位置,最后，通过整合所有通道图和相关全局线索之间的特征，使用生成的完全注意相似性来重新加权每个通道图。</p>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20241111114639873.png"
                      alt="image-20241111114639873"
                ></p>
<p>为了避免增加额外的计算负担，我们尝试利用全局平均池化结果作为全局上下文先验，将空间交互引入通道NL机制。</p>
<p>具体计算过程：</p>
<p>给定输入特征图 Fin ∈ RC×H×W ，其中 C 是通道数，H 和 W 是输入张量的空间维度。首先，我们将 Fin 输入到底部的两个平行路径（即构建），每个路径都包含一个全局平均池化层，后面跟着一个线性层。在选择池化窗口的大小时，我们考虑了以下两个方面。首先，为了获得更丰富的全局上下文先验，我们选择在高度和宽度方向上使用不相等的全局池化大小而不是内核像3×3这样的窗口。其次，为了确保每个空间位置都与具有相同水平或垂直坐标的相应全局先验连接，即在计算通道关系时保持空间一致性，我们选择保留一维的长度持续的。因此，我们在这两个路径中分别采用大小为 H × 1 和 1 × W 的池化窗口。这给出 ˆ Qw ∈ RC×1×W 和 ˆ Qh ∈ RC×H×1。之后，我们重复 ^ Qw 和 ^ Qh 形成全局特征 Qw ∈ RC×H×W 和 Qh ∈ RC×H×W 。请注意，Qw和Qh分别表示水平和垂直方向上的全局先验，它们将用于实现相应维度上的空间交互。此外，我们沿 H 维度切割 Qw，从中我们可以生成一组大小为 RC×W 的 H 切片。同样，我们沿着W维度切割Qh。然后我们合并这两个组以形成最终的全局上下文 Q ∈ R(H+W )×C×S。剪切和合并操作如图3 所示。同时，我们沿 H 维度切割输入特征 Fin，产生一组大小为 RC×W 的 H 切片。同样，我们沿着 W 维度进行此操作。与 Q 的合并过程一样，这两组被整合形成特征 K ∈ R(H+W )×S×C 。以同样的方式，我们可以生成特征图V ∈ R(H+W )×C×S。</p>
<h3 id="贡献"><a href="#贡献" class="headerlink" title="贡献:"></a>贡献:</h3><ul>
<li>发现非局部自注意力方法中存在注意力缺失问题，这会损害特征表示的完整性。 </li>
<li>我们将自注意力机制重新表述为完全注意力方式，以生成密集且全面的特征依赖关系，从而有效且高效地解决注意力缺失问题.</li>
<li>实验广泛。</li>
</ul>
<h3 id="不足"><a href="#不足" class="headerlink" title="不足:"></a>不足:</h3><p>极高的计算量限制了其实际应用.</p>
]]></content>
      <tags>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>VAE</title>
    <url>/2024/11/10/VAE/</url>
    <content><![CDATA[<p>VAE 是一种生成式人工智能算法，它利用深度学习生成新内容、检测异常并去除噪音。</p>
<p>适合生成用于训练其他人工智能算法的合成时间序列数据，也适用于生成文本、图像和视频。不过，在生成不同类型的内容，它们更有可能成为其他模型的补充，如 GAN、稳定扩散（扩散模型的一种创新）和转换器。</p>
<p>VAE结合了两种类型的神经网络，一个网络（编码器）能够找到将原始数据编码到潜在空间的更好方法，第二个网络（解码器）能找到将这些潜在表征转化成新内容的更好方法。GAN类似，第一个神经网络找到生成虚假内容的更好方法，第二个神经网络找到检测虚假内容的更好方法。</p>
<h3 id="自动编码器介绍"><a href="#自动编码器介绍" class="headerlink" title="自动编码器介绍"></a>自动编码器介绍</h3><p>自动编码器有助于创建用于压缩数据和检测异常的编解码器。早期应用于降维和特征学习，目前可集成到其他AI或机器学习算法中提高精度和性能。（比如提取人声和音乐）</p>
<h3 id="自动编码器的类型"><a href="#自动编码器的类型" class="headerlink" title="自动编码器的类型"></a>自动编码器的类型</h3><p>基本自动编码器有几种类型，包括以下几种：</p>
<ul>
<li><strong>稀疏自动编码器。</strong>这是最古老和最流行的方法之一。它们适用于<strong>特征提取、降维、异常检测和迁移学习</strong>。它们使用技术来鼓励神经网络仅使用中间神经元的子集。这些未使用的神经元的剩余部分使它们能够灵活地识别和学习更有效的数据表示。</li>
<li><strong>去噪自动编码器</strong>。它们学习<strong>从嘈杂的数据流中重建原始数据</strong>的方法。它们通常用于清理低光图像、识别语音和预处理物联网数据。</li>
<li><strong>收缩式自动编码器。</strong>它们专门学习一种能够<strong>适应输入数据细微变化</strong>的表示。这有助于它们更好地适应未见数据。研究人员使用它们通过突出显示数据集中负责结果的最显着特征来提高神经网络模型的可解释性。</li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>操作系统笔记</title>
    <url>/2024/10/20/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/</url>
    <content><![CDATA[<p>想认真学习一下，以下是看jyy老师的操作系统的笔记：</p>
<h2 id="01-操作系统概述"><a href="#01-操作系统概述" class="headerlink" title="01-操作系统概述"></a>01-操作系统概述</h2><p>首先要思考为什么要学“任何东西”，这个思维很开阔，很有启发</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.assets/image-20241020162250371.png"
                      alt="image-20241020162250371"
                ></p>
<p>由此可以思考为什么要学操作系统</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.assets/image-20241020162944094.png"
                      alt="image-20241020162944094"
                ></p>
<p>操作系统是如何变的？</p>
<ul>
<li>三个重要的线索：硬件（计算机）、软件（程序）、操作系统（管理硬件和软件的软件）</li>
<li>操作系统是<strong>管理软硬件资源、为程序提供服务</strong>的程序。</li>
</ul>
<h3 id="从历史时间线看操作系统的变化"><a href="#从历史时间线看操作系统的变化" class="headerlink" title="从历史时间线看操作系统的变化"></a>从历史时间线看操作系统的变化</h3><h4 id="1946年"><a href="#1946年" class="headerlink" title="1946年"></a>1946年</h4><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.assets/image-20241020165419247.png"
                      alt="image-20241020165419247"
                ></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.assets/image-20241020165538123.png"
                      alt="image-20241020165538123"
                ></p>
<p>把机械波放水银里，1振动，0不振动</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.assets/image-20241020165710727.png"
                      alt="image-20241020165710727"
                ></p>
<p>那个时候没有操作系统，也没有编程语言，直接写指令操作硬件。</p>
<h4 id="1950s-1960s"><a href="#1950s-1960s" class="headerlink" title="1950s-1960s"></a>1950s-1960s</h4><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.assets/image-20241020165931889.png"
                      alt="image-20241020165931889"
                ></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.assets/image-20241020170205720.png"
                      alt="image-20241020170205720"
                ></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.assets/image-20241020170338527.png"
                      alt="image-20241020170338527"
                ></p>
<p>CTSS</p>
<ul>
<li>操作系统出现了各类对象：设备、文件、任务……</li>
</ul>
<h4 id="1960s-1970s"><a href="#1960s-1970s" class="headerlink" title="1960s-1970s"></a>1960s-1970s</h4><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.assets/image-20241020170557503.png"
                      alt="image-20241020170557503"
                ></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.assets/image-20241020170639421.png"
                      alt="image-20241020170639421"
                ></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.assets/image-20241020170650885.png"
                      alt="image-20241020170650885"
                ></p>
<p>资源隔离开来，就有了进程管理</p>
<h4 id="1970s"><a href="#1970s" class="headerlink" title="1970s+"></a>1970s+</h4><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.assets/image-20241020170801624.png"
                      alt="image-20241020170801624"
                ></p>
<h2 id="02-应用视角的操作系统"><a href="#02-应用视角的操作系统" class="headerlink" title="02-应用视角的操作系统"></a>02-应用视角的操作系统</h2><p>编译hello world过程很复杂</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.assets/image-20241020175808290.png"
                      alt="image-20241020175808290"
                ></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.assets/image-20241020175826989.png"
                      alt="image-20241020175826989"
                ></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.assets/image-20241020180112658.png"
                      alt="image-20241020180112658"
                ></p>
<p>操作系统在应用视角来说，可以说是syscall的API</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.assets/image-20241020180922249.png"
                      alt="image-20241020180922249"
                ></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.assets/image-20241020180931606-17294189726721.png"
                      alt="image-20241020180931606"
                ></p>
<p>gdb说明c语言也是一个状态机</p>
]]></content>
      <tags>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title>git简单使用</title>
    <url>/2025/02/22/git%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<p>记录一下常用的命令，每次要用的时候都忘记</p>
]]></content>
      <tags>
        <tag>工具使用</tag>
      </tags>
  </entry>
  <entry>
    <title>CNN模块设计</title>
    <url>/2024/02/26/CNN%E6%A8%A1%E5%9D%97%E8%AE%BE%E8%AE%A1/</url>
    <content><![CDATA[<p><strong>AlexNet(2012)</strong></p>
<p>分组卷积 </p>
<p>操作：先将特征通道分组，再卷积操作限制在对应组内。</p>
<p>优点：减少了浮点运算量和参数量。</p>
<p>缺点：阻碍了通道之间的信息流，削弱了网络的表达能力。</p>
<p><strong>ShuffleNet(2018)</strong></p>
<p>通道洗牌操作：用较小的代价实现了组卷积通道间的信息交互，精度提升。</p>
<p><strong>ShuffleNetV2</strong></p>
<p>通道划分操作代替分组卷积</p>
<p><strong>MobileNet(2017)</strong></p>
<p>深度可分离卷积：将标准卷积拆分为深度卷积和点卷积，其中深度卷积为分组数等于通道数的组卷积。</p>
<p><strong>MobileNetV2(2018)</strong></p>
<p>倒置残差模块：由一个升维的点卷积、一个深度卷积、一个降维的点卷积以及残差连接组成.</p>
<p><strong>MobileNetV3(2019)</strong></p>
<p>倒置残差模块加入SE模块：引入了h-swish作为非线性激活函数，同时引入了网络搜索技术来进一步提高网络效率.</p>
<p><strong>EfficientNet(2020)</strong></p>
<p>基于 MobileNetV2模块使用网络搜索技术</p>
<p><strong>EfficientNetV2(2021)</strong></p>
<p>利用优化器的角度改进了MobileNetV3模块，并将其加入搜索空间</p>
<h2 id="原始卷积"><a href="#原始卷积" class="headerlink" title="原始卷积"></a>原始卷积</h2><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="data:image/png;base64,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"
                      alt="img"
                ></p>
<p>由图可以看到，卷积过程为：将filter与矩阵叠加，然后执行相应元素的相乘，将相乘的结果进行求和，得到输出图片的目标像素值（特征图），重复操作在所有位置上。</p>
<p>卷积核的不同可以提取不同的特征</p>
<p>代码实现：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">numpy_conv2d</span>(<span class="params">image, kernel, stride=<span class="number">1</span>, padding=<span class="string">&#x27;same&#x27;</span></span>):</span><br><span class="line">    <span class="comment"># 输入和卷积核转为NumPy数组</span></span><br><span class="line">    image = np.array(image)</span><br><span class="line">    kernel = np.array(kernel)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 处理多通道输入（RGB图像）</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(image.shape) == <span class="number">3</span>:</span><br><span class="line">        <span class="keyword">return</span> np.stack([numpy_conv2d(image[:,:,c], kernel) <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(image.shape[<span class="number">2</span>])], axis=<span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 添加padding</span></span><br><span class="line">    k_h, k_w = kernel.shape</span><br><span class="line">    <span class="keyword">if</span> padding == <span class="string">&#x27;same&#x27;</span>:</span><br><span class="line">        pad_h = (k_h - <span class="number">1</span>) // <span class="number">2</span></span><br><span class="line">        pad_w = (k_w - <span class="number">1</span>) // <span class="number">2</span></span><br><span class="line">        image = np.pad(image, ((pad_h, pad_h), (pad_w, pad_w)), mode=<span class="string">&#x27;constant&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算输出尺寸</span></span><br><span class="line">    out_h = (image.shape[<span class="number">0</span>] - k_h) // stride + <span class="number">1</span></span><br><span class="line">    out_w = (image.shape[<span class="number">1</span>] - k_w) // stride + <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 初始化输出矩阵</span></span><br><span class="line">    output = np.zeros((out_h, out_w))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 向量化计算</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(out_h):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(out_w):</span><br><span class="line">            h_start = i * stride</span><br><span class="line">            w_start = j * stride</span><br><span class="line">            receptive_field = image[h_start:h_start+k_h, w_start:w_start+k_w]</span><br><span class="line">            output[i, j] = np.<span class="built_in">sum</span>(receptive_field * kernel)</span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试用例</span></span><br><span class="line">image = np.random.randn(<span class="number">224</span>, <span class="number">224</span>)  <span class="comment"># 模拟输入图像</span></span><br><span class="line">kernel = np.array([[<span class="number">1</span>, <span class="number">0</span>, -<span class="number">1</span>],      <span class="comment"># Sobel水平边缘检测核</span></span><br><span class="line">                   [<span class="number">2</span>, <span class="number">0</span>, -<span class="number">2</span>],</span><br><span class="line">                   [<span class="number">1</span>, <span class="number">0</span>, -<span class="number">1</span>]])</span><br><span class="line">result = numpy_conv2d(image, kernel, padding=<span class="string">&#x27;same&#x27;</span>)</span><br></pre></td></tr></table></figure></div>

<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/../images/image-20250226185155473.png"
                      alt="image-20250226185155473"
                ></p>
<p>使用pytorch实现：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义标准卷积层</span></span><br><span class="line">standard_conv = nn.Conv2d(</span><br><span class="line">    in_channels=<span class="number">3</span>,      <span class="comment"># 输入通道数</span></span><br><span class="line">    out_channels=<span class="number">64</span>,    <span class="comment"># 输出通道数</span></span><br><span class="line">    kernel_size=<span class="number">3</span>,      <span class="comment"># 卷积核尺寸 (3x3)</span></span><br><span class="line">    stride=<span class="number">1</span>,           <span class="comment"># 步长</span></span><br><span class="line">    padding=<span class="number">1</span>,          <span class="comment"># 填充</span></span><br><span class="line">    bias=<span class="literal">False</span>          <span class="comment"># 是否使用偏置项</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入数据 (Batch=4, Channels=3, Height=224, Width=224)</span></span><br><span class="line">x = torch.randn(<span class="number">4</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line">output = standard_conv(x)</span><br><span class="line"><span class="built_in">print</span>(output.shape)  <span class="comment"># 输出形状: [4, 64, 224, 224]</span></span><br></pre></td></tr></table></figure></div>

<h2 id="分组卷积"><a href="#分组卷积" class="headerlink" title="分组卷积"></a>分组卷积</h2><p>特性：参数量减少，是普通卷积的1&#x2F;group倍。用来降低参数量，就是速度会减慢。</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#分4组</span></span><br><span class="line">group_conv = nn.Conv2d(</span><br><span class="line">    in_channels=<span class="number">64</span>,</span><br><span class="line">    out_channels=<span class="number">64</span>,  <span class="comment"># 输出通道数必须能被 groups 整除</span></span><br><span class="line">    kernel_size=<span class="number">3</span>,</span><br><span class="line">    stride=<span class="number">1</span>,</span><br><span class="line">    padding=<span class="number">1</span>,</span><br><span class="line">    groups=<span class="number">4</span>,          <span class="comment"># 关键参数：分4组</span></span><br><span class="line">    bias=<span class="literal">False</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">x = torch.randn(<span class="number">4</span>, <span class="number">64</span>, <span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line">output = group_conv(x)</span><br><span class="line"><span class="built_in">print</span>(output.shape)  <span class="comment"># 输出形状: [4, 64, 224, 224]</span></span><br></pre></td></tr></table></figure></div>



<h2 id="深度可分离卷积"><a href="#深度可分离卷积" class="headerlink" title="深度可分离卷积"></a>深度可分离卷积</h2><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DepthwiseSeparableConv</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 深度卷积（每个输入通道单独卷积）</span></span><br><span class="line">        self.depthwise = nn.Conv2d(</span><br><span class="line">            in_channels, </span><br><span class="line">            in_channels,  <span class="comment"># 输出通道数 = 输入通道数</span></span><br><span class="line">            kernel_size=<span class="number">3</span>,</span><br><span class="line">            padding=<span class="number">1</span>,</span><br><span class="line">            groups=in_channels,  <span class="comment"># 关键：groups=输入通道数</span></span><br><span class="line">            bias=<span class="literal">False</span></span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 逐点卷积（1x1卷积组合通道信息）</span></span><br><span class="line">        self.pointwise = nn.Conv2d(</span><br><span class="line">            in_channels,</span><br><span class="line">            out_channels,</span><br><span class="line">            kernel_size=<span class="number">1</span>,  <span class="comment"># 1x1卷积核</span></span><br><span class="line">            bias=<span class="literal">False</span></span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.depthwise(x)</span><br><span class="line">        x = self.pointwise(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用示例</span></span><br><span class="line">ds_conv = DepthwiseSeparableConv(in_channels=<span class="number">64</span>, out_channels=<span class="number">128</span>)</span><br><span class="line">x = torch.randn(<span class="number">4</span>, <span class="number">64</span>, <span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line">output = ds_conv(x)</span><br><span class="line"><span class="built_in">print</span>(output.shape)  <span class="comment"># 输出形状: [4, 128, 224, 224]</span></span><br></pre></td></tr></table></figure></div>

]]></content>
  </entry>
  <entry>
    <title>实时语义分割</title>
    <url>/2025/02/20/%E5%AE%9E%E6%97%B6%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/</url>
    <content><![CDATA[<p>k看了一篇24年的综述，所以整理了一下</p>
<p>链接：<a class="link"   href="http://cjig.cn/zh/article/doi/10.11834/jig.230659/" >http://cjig.cn/zh/article/doi/10.11834/jig.230659/ <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<p>代码：<a class="link"   href="https://github.com/xzz777/Awesome-Real-time-Semantic-Segmentation" >https://github.com/xzz777/Awesome-Real-time-Semantic-Segmentation <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<hr>
<p>传统图像分割</p>
<p>包括多种基于区域和基于边界的算法</p>
<p>OTSU法、K均值聚类、分水岭、区域生长法、活动轮廓、图割法、条件随机场、马尔代夫随机场等。</p>
<hr>
<p>提出深度学习之后提出了</p>
<p>全卷积网络FCN</p>
<p>卷积神经网络CNN</p>
<p>–&gt;需要恢复下采样损失的信息</p>
<p>–&gt;多尺度的特征信息、长距离的上下文（提升精度显著）</p>
<p>–&gt;丰富上下文信息的方法：扩大感受野、多尺度融合、自注意力机制</p>
<ul>
<li>更好的特征提取网络</li>
</ul>
<p>VGG(2015)</p>
<p>GoogLeNet(2015)</p>
<p>ResNet(2016)</p>
<p>HRNet(2019)</p>
<ul>
<li>更好的上下文捕获方法</li>
</ul>
<p>U-Net(2015)</p>
<p>PSPNet(2017)</p>
<p>RefineNet(2017)</p>
<ul>
<li>添加注意力模块的CNN</li>
</ul>
<hr>
<p>Transformer被提出之后引入cv领域</p>
<p>SETR(2021)  首次将视觉transformer应用到图像分割</p>
<p>PVT(2021)  将常用的特征金字塔架构引入基于transformer图像分割的模型</p>
<p>SegFormer(2021) 提出一个简洁、高效且多尺度的TransFormer图像分割模型</p>
<p>Swin Transformer(2021) 可代替CNN</p>
<p>–&gt;精度显著提升，但是带来了高额的计算代价；尤其是自注意力机制和自注意力机制为核心的transformer网络，虽然有全局建模能力，被证实非常适合捕获长距离上下文，但是与图像分辨率呈平方复杂度，显著增大了语义分割模型的推理延迟</p>
<p>一般而言，实时语义分割网络是指在指定设备上，推理时的帧率能够达到30帧&#x2F;s及以上（即人眼对视频流畅的最低帧率要求）的语义分割网络。</p>
<hr>
<p>挑战：</p>
<p>为了得到更高的分割精度，语义分割模型同时需要丰富的空间细节信息和多尺度上下文信息。然而，一方面，丰富的空间细节信息需要保留高分辨率的底层特征图，这会极大地增加计算代价；另一方面，多尺度上下文的捕获和融合又需要设计复杂的模块和交互，这会增加推理的延迟。如何以更少的计算代价，保留更丰富的空间信息、捕获更有效的多尺度上下文，以得到更好的模型速度—精度平衡，是实时分割领域一直以来的挑战和领域内研究者们一直以来的追求。此外，在一些资源受限的移动设备和边缘设备上，模型的大小和内存占用量也显得至关重要，在这些设备上的实时语义分割网络如何进行优化设计也是实时分割领域面临的一项挑战。</p>
<hr>
<ul>
<li>单分支网络</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/../images/image-20250226185909970.png"
                      alt="image-20250226185909970"
                ></p>
<p>（1）解码器利用编码器最后的输出进行特征回复和解码（代表：ENet）</p>
<p>（2）利用多尺度特征进行简单拼接融合（代表：SegFormer和SegNext）</p>
<p>（3）舍弃解码器，直接对编码器特征进行分类输出</p>
<p>ENet：在前两个模块进行下采样，使用更小的特征图进行后续操作。</p>
<p>认为解码器的作用是对编码器的输出进行上采样，只对细节进行微调，编码器本身应具有信息处理和过滤的作用。</p>
<p>优点：推理速度显著提升</p>
<p>缺点：精度较低</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/../images/image-20250226185933122.png"
                      alt="image-20250226185933122"
                ></p>
<hr>
<p>实时语义分割研究方案主要分为三类：</p>
<p>１）设计轻量化模块结构，如DUpsampling模块、ERF-PSPNet采用的残差分解卷积模块等；</p>
<p>２）设计新型网络设计范式，如ICNet 和 BiSeNet采用的多支路进行信息补充的结构、将超分辨率算法引入指导低分辨率图像语义分割的方式、利用知识蒸馏指导实时语义分割网络的训练等；</p>
<p>３）采用轻量级基础网络提取低级特征信息，如SwiftNet和DFANet等。</p>
]]></content>
  </entry>
  <entry>
    <title>ModelCompression.md</title>
    <url>/2025/02/26/ModelCompression/</url>
    <content><![CDATA[<h2 id="模型压缩通用方法"><a href="#模型压缩通用方法" class="headerlink" title="模型压缩通用方法"></a>模型压缩通用方法</h2><p>模型压缩是指将原本的大网络模型通过一些技术手段，压缩成为具有更好的实时性或参数量更小的模型。常见的模型压缩技术包括网络剪枝、神经架构搜索、知识蒸馏和量化。</p>
<p><strong>网络剪枝（network pruning）</strong>是指去掉网络模型中不必要的参数。网络剪枝的一般步骤是：训练一个大网络、评估每个参数的重要性、去掉不重要的参数以及微调去掉参数后的网络以恢复剪枝损失的部分精度。剪枝可以利用大模型本身容易训练到较高精度的优势，以最小的精度损失代价来获得更小的模型。</p>
<ul>
<li>如何确定要保留什么结构以及修剪哪些结构？</li>
</ul>
<p>1.可以修剪绝对值（或幅度）最小的权重。（属于非结构化剪枝，无法加速稀疏矩阵计算）</p>
<p>2.根据过滤器的范数（L1或者L2）对过滤器进行排序（？）</p>
<p>3.在要修剪的每组图层之后为每个特征图插入一个可学习的乘法参数，当参数减少到0时，有效修剪了负责这个通道的整套参数，这个参数的大小说明了所有参数的重要性。</p>
<p>4.在小批量训练数据上累积梯度，并根据该梯度与每个参数的相应权重之间的乘积进行修剪。</p>
<p><strong>知识蒸馏（knowledge distillation）</strong>利用大型教师模型网络参数包含的知识监督小型学生模型，使其能够在一定程度上拟合大的教师模型的输出，从而提高学生模型的精度，以得到更高精度的紧凑小模型。</p>
<p><strong>量化（quantization）</strong>是指通过一定技术手段降低模型的数字精度以达到压缩模型、加快推理速度的效果，是模型部署常用的技术之一。</p>
<p>接下来使用一个简单的CNN模型为例学习</p>
<p>剪枝原理：</p>
<p>剪枝基于权重的重要性进行，例中使用了L1范数作为重要性度量</p>
<p>将不重要的权重设置为0，从而减少模型计算量</p>
<p>剪枝后的模型需要微调来恢复性能</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.utils.prune <span class="keyword">as</span> prune</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleCNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleCNN, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">16</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">32</span> * <span class="number">8</span> * <span class="number">8</span>, <span class="number">10</span>)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.pool(self.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool(self.relu(self.conv2(x)))</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">32</span> * <span class="number">8</span> * <span class="number">8</span>)</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">count_parameters</span>(<span class="params">model</span>):</span><br><span class="line">    total_params = <span class="number">0</span></span><br><span class="line">    zero_params = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> name, module <span class="keyword">in</span> model.named_modules():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(module, (nn.Conv2d, nn.Linear)):</span><br><span class="line">            <span class="comment"># 检查是否已经被剪枝</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">hasattr</span>(module, <span class="string">&#x27;weight_mask&#x27;</span>):</span><br><span class="line">                total_params += module.weight_mask.numel()</span><br><span class="line">                zero_params += torch.<span class="built_in">sum</span>(module.weight_mask == <span class="number">0</span>).item()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                total_params += module.weight.numel()</span><br><span class="line">                zero_params += torch.<span class="built_in">sum</span>(module.weight == <span class="number">0</span>).item()</span><br><span class="line">    <span class="keyword">return</span> total_params, zero_params</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">apply_pruning</span>(<span class="params">model, pruning_ratio</span>):</span><br><span class="line">    <span class="keyword">for</span> name, module <span class="keyword">in</span> model.named_modules():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(module, (nn.Conv2d, nn.Linear)):</span><br><span class="line">            prune.l1_unstructured(</span><br><span class="line">                module,</span><br><span class="line">                name=<span class="string">&#x27;weight&#x27;</span>,</span><br><span class="line">                amount=pruning_ratio</span><br><span class="line">            )</span><br><span class="line">            <span class="comment"># 打印每层的剪枝统计</span></span><br><span class="line">            mask = module.weight_mask</span><br><span class="line">            total = mask.numel()</span><br><span class="line">            zeros = torch.<span class="built_in">sum</span>(mask == <span class="number">0</span>).item()</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Layer <span class="subst">&#123;name&#125;</span>:&quot;</span>)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;总参数: <span class="subst">&#123;total&#125;</span>&quot;</span>)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;被剪枝参数: <span class="subst">&#123;zeros&#125;</span>&quot;</span>)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;层压缩率: <span class="subst">&#123;zeros/total*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%\n&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试代码</span></span><br><span class="line">model = SimpleCNN()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;剪枝前参数统计：&quot;</span>)</span><br><span class="line">total, zeros = count_parameters(model)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;总参数数量: <span class="subst">&#123;total&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;零参数数量: <span class="subst">&#123;zeros&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;初始压缩率: <span class="subst">&#123;zeros/total*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%\n&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 应用90%的剪枝率</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;应用50%剪枝...&quot;</span>)</span><br><span class="line">apply_pruning(model, <span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n剪枝后总体参数统计：&quot;</span>)</span><br><span class="line">total, zeros = count_parameters(model)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;总参数数量: <span class="subst">&#123;total&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;零参数数量: <span class="subst">&#123;zeros&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;最终压缩率: <span class="subst">&#123;zeros/total*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%&quot;</span>)</span><br></pre></td></tr></table></figure></div>

<p>量化是将模型的权重从32位浮点数转换为低位数值（如8位整数）的过程。</p>
<p>可参考代码：<a class="link"   href="https://github.com/BastianChen/Model-Compression-Demo/tree/master/quantization" >https://github.com/BastianChen/Model-Compression-Demo/tree/master/quantization <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<p>知识蒸馏这是一种将大模型（教师模型）的知识转移到小模型（学生模型）的方法</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 教师模型（与之前的SimpleCNN相同）</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TeacherCNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(TeacherCNN, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">16</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">32</span> * <span class="number">8</span> * <span class="number">8</span>, <span class="number">10</span>)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.pool(self.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool(self.relu(self.conv2(x)))</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">32</span> * <span class="number">8</span> * <span class="number">8</span>)</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 学生模型（更小的网络）</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">StudentCNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(StudentCNN, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">8</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">8</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">8</span> * <span class="number">8</span>, <span class="number">10</span>)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.pool(self.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool(self.relu(self.conv2(x)))</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">16</span> * <span class="number">8</span> * <span class="number">8</span>)</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DistillationLoss</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, temperature=<span class="number">3.0</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(DistillationLoss, self).__init__()</span><br><span class="line">        self.temperature = temperature</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, student_outputs, teacher_outputs, labels, alpha=<span class="number">0.5</span></span>):</span><br><span class="line">        <span class="comment"># 软目标损失</span></span><br><span class="line">        soft_targets = F.softmax(teacher_outputs / self.temperature, dim=<span class="number">1</span>)</span><br><span class="line">        soft_prob = F.log_softmax(student_outputs / self.temperature, dim=<span class="number">1</span>)</span><br><span class="line">        soft_targets_loss = -torch.<span class="built_in">sum</span>(soft_targets * soft_prob) * (self.temperature ** <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 硬目标损失</span></span><br><span class="line">        hard_loss = F.cross_entropy(student_outputs, labels)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 组合损失</span></span><br><span class="line">        loss = (alpha * soft_targets_loss) + ((<span class="number">1</span> - alpha) * hard_loss)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_cifar10</span>(<span class="params">batch_size=<span class="number">128</span></span>):</span><br><span class="line">    transform = transforms.Compose([</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">    trainset = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>,</span><br><span class="line">                                            download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">    trainloader = DataLoader(trainset, batch_size=batch_size,</span><br><span class="line">                             shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    testset = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>,</span><br><span class="line">                                           download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">    testloader = DataLoader(testset, batch_size=batch_size,</span><br><span class="line">                            shuffle=<span class="literal">False</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> trainloader, testloader</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_model</span>(<span class="params">model, dataloader, device</span>):</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data, labels <span class="keyword">in</span> dataloader:</span><br><span class="line">            data, labels = data.to(device), labels.to(device)</span><br><span class="line">            outputs = model(data)</span><br><span class="line">            _, predicted = torch.<span class="built_in">max</span>(outputs.data, <span class="number">1</span>)</span><br><span class="line">            total += labels.size(<span class="number">0</span>)</span><br><span class="line">            correct += (predicted == labels).<span class="built_in">sum</span>().item()</span><br><span class="line">    <span class="keyword">return</span> <span class="number">100</span> * correct / total</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_with_distillation</span>(<span class="params">teacher_model, student_model, train_loader, test_loader,</span></span><br><span class="line"><span class="params">                            device, epochs=<span class="number">10</span>, temperature=<span class="number">3.0</span>, alpha=<span class="number">0.5</span></span>):</span><br><span class="line">    teacher_model.to(device)</span><br><span class="line">    student_model.to(device)</span><br><span class="line"></span><br><span class="line">    teacher_model.<span class="built_in">eval</span>()</span><br><span class="line">    student_model.train()</span><br><span class="line"></span><br><span class="line">    optimizer = torch.optim.Adam(student_model.parameters())</span><br><span class="line">    distillation_criterion = DistillationLoss(temperature=temperature)</span><br><span class="line"></span><br><span class="line">    best_acc = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        running_loss = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> batch_idx, (data, target) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">            data, target = data.to(device), target.to(device)</span><br><span class="line"></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 获取教师模型的输出</span></span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                teacher_output = teacher_model(data)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 获取学生模型的输出</span></span><br><span class="line">            student_output = student_model(data)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 计算蒸馏损失</span></span><br><span class="line">            loss = distillation_criterion(student_output, teacher_output,</span><br><span class="line">                                          target, alpha=alpha)</span><br><span class="line"></span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">            running_loss += loss.item()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> batch_idx % <span class="number">100</span> == <span class="number">99</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&#x27;Epoch: <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, Batch: <span class="subst">&#123;batch_idx + <span class="number">1</span>&#125;</span>, &#x27;</span></span><br><span class="line">                      <span class="string">f&#x27;Loss: <span class="subst">&#123;running_loss / <span class="number">100</span>:<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">                running_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 评估模型</span></span><br><span class="line">        student_acc = evaluate_model(student_model, test_loader, device)</span><br><span class="line">        teacher_acc = evaluate_model(teacher_model, test_loader, device)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;\nEpoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>:&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Teacher Accuracy: <span class="subst">&#123;teacher_acc:<span class="number">.2</span>f&#125;</span>%&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Student Accuracy: <span class="subst">&#123;student_acc:<span class="number">.2</span>f&#125;</span>%&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> student_acc &gt; best_acc:</span><br><span class="line">            best_acc = student_acc</span><br><span class="line">            torch.save(student_model.state_dict(), <span class="string">&#x27;best_student_model.pth&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> student_model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用示例</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 加载数据</span></span><br><span class="line">    train_loader, test_loader = load_cifar10()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建模型</span></span><br><span class="line">    teacher_model = TeacherCNN()</span><br><span class="line">    student_model = StudentCNN()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练教师模型（这里假设已经训练好了）</span></span><br><span class="line">    <span class="comment"># 实际使用时需要先训练教师模型</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 进行知识蒸馏</span></span><br><span class="line">    student_model = train_with_distillation(</span><br><span class="line">        teacher_model=teacher_model,</span><br><span class="line">        student_model=student_model,</span><br><span class="line">        train_loader=train_loader,</span><br><span class="line">        test_loader=test_loader,</span><br><span class="line">        device=device,</span><br><span class="line">        epochs=<span class="number">10</span>,</span><br><span class="line">        temperature=<span class="number">3.0</span>,</span><br><span class="line">        alpha=<span class="number">0.5</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 比较模型大小</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">count_parameters</span>(<span class="params">model</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">sum</span>(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    teacher_params = count_parameters(teacher_model)</span><br><span class="line">    student_params = count_parameters(student_model)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\n模型大小比较：&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;教师模型参数数量: <span class="subst">&#123;teacher_params:,&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;学生模型参数数量: <span class="subst">&#123;student_params:,&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;压缩率: <span class="subst">&#123;(<span class="number">1</span> - student_params / teacher_params) * <span class="number">100</span>:<span class="number">.2</span>f&#125;</span>% &quot;</span>)</span><br></pre></td></tr></table></figure></div>

<p>优缺点：</p>
<ol>
<li>剪枝</li>
</ol>
<ul>
<li>优点：实现简单，可以显著减少模型大小</li>
<li>缺点：可能需要反复尝试以找到最佳剪枝比例</li>
</ul>
<ol>
<li>量化</li>
</ol>
<ul>
<li>优点：显著减少模型存储空间和推理时间</li>
<li>缺点：可能导致精度轻微下降</li>
</ul>
<ol>
<li>知识蒸馏</li>
</ol>
<ul>
<li>优点：可以得到更小但性能相近的模型</li>
<li>缺点：需要训练过程，实现相对复杂</li>
</ul>
<p> \4.  NAS神经架构搜索</p>
<p><strong>优点：</strong></p>
<ul>
<li>自动化设计，减少人工干预。</li>
<li>可能发现人类难以设计的高性能架构。</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>计算成本高，需要大量计算资源。</li>
<li>搜索过程耗时。</li>
</ul>
<p><strong>神经架构搜索（neural architecture search， NAS）</strong>是一种利用强化学习方法同时学习模型架构和相应参数的方法。简单来说，就是在一个定义好的搜索空间内，通过一定的搜索策略，得到最终表现最好的网络。通过加入准确率、推理延迟等指标，网络架构搜索产生的网络结构在轻量化应用中能获得更高的竞争力。</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义搜索空间</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ConvBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels, kernel_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(ConvBlock, self).__init__()</span><br><span class="line">        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, padding=kernel_size//<span class="number">2</span>)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.pool(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义候选架构</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_model</span>(<span class="params">kernel_size, channels</span>):</span><br><span class="line">    model = nn.Sequential(</span><br><span class="line">        ConvBlock(<span class="number">1</span>, channels[<span class="number">0</span>], kernel_size),</span><br><span class="line">        ConvBlock(channels[<span class="number">0</span>], channels[<span class="number">1</span>], kernel_size),</span><br><span class="line">        nn.Flatten(),</span><br><span class="line">        nn.Linear(channels[<span class="number">1</span>] * <span class="number">7</span> * <span class="number">7</span>, <span class="number">10</span>)  <span class="comment"># 假设输入是 28x28</span></span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据</span></span><br><span class="line">transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,))])</span><br><span class="line">train_data = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">train_loader = DataLoader(train_data, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机搜索策略</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">random_search</span>(<span class="params">search_space, num_trials=<span class="number">10</span></span>):</span><br><span class="line">    best_accuracy = <span class="number">0</span></span><br><span class="line">    best_model = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_trials):</span><br><span class="line">        <span class="comment"># 随机选择超参数</span></span><br><span class="line">        kernel_size = search_space[<span class="string">&#x27;kernel_sizes&#x27;</span>][torch.randint(<span class="number">0</span>, <span class="built_in">len</span>(search_space[<span class="string">&#x27;kernel_sizes&#x27;</span>]), (<span class="number">1</span>,)).item()]</span><br><span class="line">        channels = [search_space[<span class="string">&#x27;channels&#x27;</span>][torch.randint(<span class="number">0</span>, <span class="built_in">len</span>(search_space[<span class="string">&#x27;channels&#x27;</span>]), (<span class="number">1</span>,)).item() <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>)]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 创建模型</span></span><br><span class="line">        model = create_model(kernel_size, channels)</span><br><span class="line">        optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line">        criterion = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 训练模型</span></span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):  <span class="comment"># 简单训练 2 个 epoch</span></span><br><span class="line">            <span class="keyword">for</span> inputs, labels <span class="keyword">in</span> train_loader:</span><br><span class="line">                optimizer.zero_grad()</span><br><span class="line">                outputs = model(inputs)</span><br><span class="line">                loss = criterion(outputs, labels)</span><br><span class="line">                loss.backward()</span><br><span class="line">                optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 评估模型</span></span><br><span class="line">        correct = <span class="number">0</span></span><br><span class="line">        total = <span class="number">0</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="keyword">for</span> inputs, labels <span class="keyword">in</span> train_loader:</span><br><span class="line">                outputs = model(inputs)</span><br><span class="line">                _, predicted = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)</span><br><span class="line">                total += labels.size(<span class="number">0</span>)</span><br><span class="line">                correct += (predicted == labels).<span class="built_in">sum</span>().item()</span><br><span class="line">        accuracy = correct / total</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 更新最佳模型</span></span><br><span class="line">        <span class="keyword">if</span> accuracy &gt; best_accuracy:</span><br><span class="line">            best_accuracy = accuracy</span><br><span class="line">            best_model = model</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> best_model, best_accuracy</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义搜索空间</span></span><br><span class="line">search_space = &#123;</span><br><span class="line">    <span class="string">&#x27;kernel_sizes&#x27;</span>: [<span class="number">3</span>, <span class="number">5</span>],</span><br><span class="line">    <span class="string">&#x27;channels&#x27;</span>: [<span class="number">16</span>, <span class="number">32</span>, <span class="number">64</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行随机搜索</span></span><br><span class="line">best_model, best_accuracy = random_search(search_space)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Best Accuracy: <span class="subst">&#123;best_accuracy:<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(best_model)</span><br></pre></td></tr></table></figure></div>

]]></content>
  </entry>
</search>
