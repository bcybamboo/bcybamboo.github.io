<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Yolov10小知识</title>
    <url>/2024/05/26/Yolov10%E5%B0%8F%E7%9F%A5%E8%AF%86/</url>
    <content><![CDATA[<p> 今天突然发现都出了YOLOv10了，技术真的变更的好快，以下是本人学习的一些小知识。</p>
<p>为了提升性能效率边界，这一次改进的是后处理和模型架构。</p>
<p>在原来，YOLO后处理依赖着非极大值抑制（NMS），性能对NMS的超参数敏感，阻碍了YOLO的端到端部署，产生了一定的推理延迟。</p>
<p>增强特征提取能力可以考虑以下模块：DarkNet、CSPNet、EfficientRep和ELAN等</p>
<p>DarkNet是yolov3里面的backbone，主要是由重复堆叠下采样卷积+n*残差块组成。主要是由cnn卷积核提取特征。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/14b426a4f6b44003b64dd82a14daa2d7.png"
                      alt="图片"
                ></p>
<p>CSPNet的主要目的是使该体系结构能够实现更丰富的梯度组合，同时减少计算量。</p>
<p>增强多尺度特征融合：PAN、BiC、GD和RepGFPN等</p>
<p>模型缩放策略和重新参数化技术</p>
]]></content>
      <tags>
        <tag>Object Detection</tag>
      </tags>
  </entry>
  <entry>
    <title>进程调度算法</title>
    <url>/2024/08/28/%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<h2 id="先来先服务调度算法FCFS"><a href="#先来先服务调度算法FCFS" class="headerlink" title="先来先服务调度算法FCFS"></a>先来先服务调度算法FCFS</h2><p><strong>每次从就绪队列选择最先进入队列的进程，然后一直运行，直到进程退出或被阻塞，才会继续从队列中选择第一个进程接着运行。</strong></p>
<p>对短作业不利。</p>
<p>适用于 CPU 繁忙型作业的系统，而不适用于 I&#x2F;O 繁忙型作业的系统。</p>
<h2 id="最短作业优先调度算法SJF"><a href="#最短作业优先调度算法SJF" class="headerlink" title="最短作业优先调度算法SJF"></a>最短作业优先调度算法SJF</h2><p><strong>优先选择运行时间最短的进程来运行</strong>，这有助于提高系统的吞吐量。</p>
<p>对长作业不利，被推迟到很后面。</p>
<h2 id="高响应比优先调度算法HRRN"><a href="#高响应比优先调度算法HRRN" class="headerlink" title="高响应比优先调度算法HRRN"></a>高响应比优先调度算法HRRN</h2><p>进程调度先运行响应比优先级最高的。<br>$$<br>优先权&#x3D; \frac { 等待时间+要求服务时间 }{ 要求服务时间 }<br>$$<br>兼顾长作业和短作业。</p>
<h2 id="时间片轮转调度算法RR"><a href="#时间片轮转调度算法RR" class="headerlink" title="时间片轮转调度算法RR"></a>时间片轮转调度算法RR</h2><p><strong>使用最广</strong></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20240828124832086.png"
                      alt="image-20240828124832086"
                ></p>
<p>每个进程被分配一个时间段，称为时间片，即在该时间内可以为该进程运行。</p>
<ul>
<li>如果时间片用完，进程还在运行，那么将会把此进程从 CPU 释放出来，并把 CPU 分配另外一个进程；</li>
<li>如果该进程在时间片结束前阻塞或结束，则 CPU 立即进行切换；</li>
</ul>
<p>另外，时间片的长度就是一个很关键的点：</p>
<ul>
<li>如果时间片设得太短会导致过多的进程上下文切换，降低了 CPU 效率；</li>
<li>如果设得太长又可能引起对短作业进程的响应时间变长。将</li>
</ul>
<p>通常时间片设为 <code>20ms~50ms</code> 通常是一个比较合理的折中值。</p>
]]></content>
      <tags>
        <tag>操作系统</tag>
        <tag>数模</tag>
      </tags>
  </entry>
  <entry>
    <title>about_conda</title>
    <url>/2024/07/09/about-conda/</url>
    <content><![CDATA[<p>记录一下常用的conda命令</p>
<p>查看版本</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda --version</span><br></pre></td></tr></table></figure></div>

<p>查看环境配置</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda config --show</span><br></pre></td></tr></table></figure></div>

<p>更新conda</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda update conda</span><br></pre></td></tr></table></figure></div>

<p>创建虚拟环境</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda create -n env_name python=3.9</span><br></pre></td></tr></table></figure></div>

<p>查看虚拟环境</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda env list</span><br><span class="line">conda info -e</span><br><span class="line">conda info --envs</span><br></pre></td></tr></table></figure></div>

<p>激活虚拟环境</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda activate env_name</span><br></pre></td></tr></table></figure></div>

<p>退出虚拟环境</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda deactivate</span><br></pre></td></tr></table></figure></div>

<p>删除虚拟环境</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda remove --name env_name --all</span><br></pre></td></tr></table></figure></div>

<p>删除虚拟环境中的某些包</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda remove --name env_name  package_name</span><br></pre></td></tr></table></figure></div>

<p>查看环境中有的包</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda list</span><br></pre></td></tr></table></figure></div>

<p>克隆虚拟环境</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda create -n BBB --clone AAA</span><br></pre></td></tr></table></figure></div>

<p>在本地的conda环境AAA，克隆给新环境BBB</p>
]]></content>
  </entry>
  <entry>
    <title>如何读论文</title>
    <url>/2024/02/16/begin/</url>
    <content><![CDATA[<h1 id="如何读论文"><a href="#如何读论文" class="headerlink" title="如何读论文"></a>如何读论文</h1><p>从头开始看一篇论文所花的时间太久了，而且记忆不深，就没什么效果（对我来说），所以在参考了学长的观点后，我觉得可以从以下结构开始阅读：</p>
<table>
<thead>
<tr>
<th align="center">原来结构</th>
<th align="center">推荐结构</th>
</tr>
</thead>
<tbody><tr>
<td align="center">Abstract</td>
<td align="center">Abstract(摘要)</td>
</tr>
<tr>
<td align="center">Introduction</td>
<td align="center">Discussion（讨论）</td>
</tr>
<tr>
<td align="center">Methods</td>
<td align="center">Introduction（导言）</td>
</tr>
<tr>
<td align="center">Results</td>
<td align="center">Results（结果）</td>
</tr>
<tr>
<td align="center">Discussion</td>
<td align="center">Methods（方法）</td>
</tr>
</tbody></table>
<h2 id="Abstract-摘要"><a href="#Abstract-摘要" class="headerlink" title="Abstract(摘要)"></a>Abstract(摘要)</h2><p>关注四个信息：</p>
<ul>
<li>研究目的（为什么要研究）</li>
<li>方法（如何研究）</li>
<li>结果（发现了什么）</li>
<li>结论（它意味着什么）</li>
</ul>
<h2 id="Discussion（讨论）"><a href="#Discussion（讨论）" class="headerlink" title="Discussion（讨论）"></a>Discussion（讨论）</h2><p>内容一般是：</p>
<ul>
<li>明确回答introduction中提出的问题</li>
<li>解释结果如何支持结论</li>
</ul>
<p>看看自己是否理解和相信作者的观点</p>
<h2 id="Introduction（导言）"><a href="#Introduction（导言）" class="headerlink" title="Introduction（导言）"></a>Introduction（导言）</h2><p>作用：</p>
<ul>
<li>激发我们对主题的兴趣</li>
<li>将文章置于大背景中</li>
</ul>
<p>一般来说，先引导作者从一般问题（对主题的已知了解）到具体问题（对主题的未知了解），再到重点问题（作者提出的问题）。所以要介绍之前的作品以及这些作品与该主题的关系。</p>
<p>想想作者为什么要做这个研究，研究的问题和讨论的问题是否一致</p>
<h2 id="Results（结果）"><a href="#Results（结果）" class="headerlink" title="Results（结果）"></a>Results（结果）</h2><p>内容一般是：</p>
<ul>
<li>作者的发现</li>
<li>关键数据，通常用图表显示</li>
</ul>
<p>问问自己所收集的数据是否适合回答所研究的问题</p>
<h2 id="Methods（方法）"><a href="#Methods（方法）" class="headerlink" title="Methods（方法）"></a>Methods（方法）</h2><ul>
<li>做了哪些实验来回答引言中提出的问题</li>
</ul>
<h2 id="如何找到文章的要点"><a href="#如何找到文章的要点" class="headerlink" title="如何找到文章的要点"></a>如何找到文章的要点</h2><p>主要地方包括：</p>
<ul>
<li><p>文章标题</p>
</li>
<li><p>Abstract</p>
</li>
<li><p>Keywords</p>
<ul>
<li>“We hypothesize that…”（假设）</li>
<li>“We propose…”(建议)</li>
<li>“We introduce…”（提出）</li>
</ul>
</li>
<li><p>图表的标题</p>
</li>
<li><p>introduction的第一句和最后一句</p>
</li>
</ul>
<h2 id="notes"><a href="#notes" class="headerlink" title="notes"></a>notes</h2><p>做笔记也是阅读文献中比较重要的一步，用固定的格式比较方便查找：</p>
<ul>
<li><strong>Article title</strong>（文章标题）</li>
<li>作者，期刊</li>
<li>有关方向</li>
<li>阅读日期</li>
<li>网址</li>
<li>主要概念（推荐结构）</li>
<li>我自己的想法</li>
</ul>
]]></content>
      <tags>
        <tag>notes</tag>
      </tags>
  </entry>
  <entry>
    <title>LPSNet：Lightweight and Progressively-Scalable Networks for Semantic Segmentation</title>
    <url>/2024/03/16/LPSNet/</url>
    <content><![CDATA[<h1 id="Lightweight-and-Progressively-Scalable-Networks-for-Semantic-Segmentation"><a href="#Lightweight-and-Progressively-Scalable-Networks-for-Semantic-Segmentation" class="headerlink" title="Lightweight and Progressively-Scalable Networks for Semantic Segmentation"></a>Lightweight and Progressively-Scalable Networks for Semantic Segmentation</h1><h2 id="Abstract-摘要"><a href="#Abstract-摘要" class="headerlink" title="Abstract(摘要)"></a>Abstract(摘要)</h2><p>主要方法：通过一次性扩展单个维度（卷积块的数量、通道的数量或输入分辨率）来逐步将小型网络扩展到更大的网络，以满足最佳速度&#x2F;精度权衡。</p>
<h2 id="Conclusion-and-Discussion（总结和讨论）"><a href="#Conclusion-and-Discussion（总结和讨论）" class="headerlink" title="Conclusion and Discussion（总结和讨论）"></a>Conclusion and Discussion（总结和讨论）</h2><p>它是一种探索经济设计并逐步扩大网络以实现有效的语义分割</p>
<p>可能影响精度&#x2F;延迟的平衡的因素为：<strong>多路径框架中的基本卷积块和路径交互方式</strong></p>
<p>在卷积块中使用3×3Conv和双线性插值实现跨路径的交互</p>
<p>（上采样换成PointRend会怎样？）</p>
<p>先构建一个微型网络，然后一次性扩展单个维度将微型网络扩展为一系列较大的网络</p>
<h2 id="Introduction（引言）"><a href="#Introduction（引言）" class="headerlink" title="Introduction（引言）"></a>Introduction（引言）</h2><p>语义分割是为图像分割或视频帧的每个像素分配语义标签</p>
<p>多尺度学习沿三个不同维度进行语义分割：</p>
<ul>
<li>U-shape(结构分层融合特征，逐步提高空间分辨率)</li>
<li>pyramid pooling（在多个尺度上执行空间或空洞空间金字塔池化深入研究金字塔信息）</li>
<li>muti-path framework（将输入图像的大小调整为多个分辨率或尺度，并将每个尺度输入到深度学习的单独路径中）</li>
</ul>
<p>本文使用的就是muti-path framework，将输入分辨率从高到低并行放置，直接保持高分辨率信息，这样学习到的特征可能更有能力对每个像素进行分类和定位</p>
<ul>
<li>轻量化用于语义分割的计算单元</li>
<li>逐步扩大网络，同时平衡准确性和推理延迟</li>
</ul>
<p><strong>一次性扩展单个维度（卷积块的数量、通道的数量或输入分辨率）</strong></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/%E8%81%94%E6%83%B3%E6%88%AA%E5%9B%BE_20240315191556.png"
                      alt="联想截图_20240315191556"
                ></p>
<h3 id="contributions"><a href="#contributions" class="headerlink" title="contributions:"></a>contributions:</h3><p>(1) The lightweight design of convolutional blocks and the way of path interactions in multipath framework are shown capable of regarding as the practical principles for efficient semantic segmentation; （卷积块的轻量化设计和多路径框架中的路径交互方式实用）</p>
<p>(2) The exquisitely devised LPS-Net is shown able to progressively expand the network complexity while striking the right accuracy-efficiency tradeoff; （可扩展网络复杂性）</p>
<p>(3) LPSNet has been properly verified through extensive experiments over three datasets, and superior capability is observed on both NVIDIA GPUs and embedded devices in our experiments.（结果很不错）</p>
<h2 id="Results（结果）"><a href="#Results（结果）" class="headerlink" title="Results（结果）"></a>Results（结果）</h2><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20240315194455560.png"
                      alt="image-20240315194455560"
                ></p>
<h2 id="Methods（方法）"><a href="#Methods（方法）" class="headerlink" title="Methods（方法）"></a>Methods（方法）</h2><h3 id="Macro-Architecture（employs-the-multi-path-recipe）"><a href="#Macro-Architecture（employs-the-multi-path-recipe）" class="headerlink" title="Macro Architecture（employs the multi-path recipe）"></a>Macro Architecture（employs the multi-path recipe）</h3><p>LPS-Net 中采用多路径配方的宏架构</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20240315205546026.png"
                      alt="image-20240315205546026"
                ></p>
<p>（轻量化）三个设计原则：（卷积类型、通道数数量、跨多个路径的交互方式）</p>
<p>将输入图像的大小调整为多个尺度，并将每个尺度反馈送到单独的路径中。</p>
<p>将交互模块放置在阶段 3∼5 的末尾，旨在促进路径之间的相互交互。所有路径的输出被聚合并输入到分割头中，以生成具有 num 类通道的分数图。</p>
<p>对分数图执行双线性上采样，产生分辨率为 H×W 的输出，与输入分辨率完全匹配</p>
<h3 id="卷积块"><a href="#卷积块" class="headerlink" title="卷积块"></a>卷积块</h3><ul>
<li>卷积块的类型</li>
</ul>
<p>通过实验选择用标准卷积作为 <code>LPSNet</code> 中的构建块</p>
<ul>
<li>通道数</li>
</ul>
<p>具有 2n-divisible 通道宽度的卷积的可并行化实现。因此，将 <code>LPSNet</code> 中卷积的通道宽度设为2n-divisible，其中 <code>n</code> 尽可能大。</p>
<h3 id="Multi-Path交互"><a href="#Multi-Path交互" class="headerlink" title="Multi-Path交互"></a><strong>Multi-Path交互</strong></h3><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20240316154125443.png"
                      alt="image-20240316154125443"
                ></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20240316154135995.png"
                      alt="image-20240316154135995"
                ></p>
<h3 id="扩展算法"><a href="#扩展算法" class="headerlink" title="扩展算法"></a>扩展算法</h3><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20240316154602791.png"
                      alt="image-20240316154602791"
                ></p>
]]></content>
      <tags>
        <tag>notes</tag>
      </tags>
  </entry>
  <entry>
    <title>细碎知识点</title>
    <url>/2024/04/24/%E7%BB%86%E7%A2%8E%E7%9F%A5%E8%AF%86%E7%82%B9/</url>
    <content><![CDATA[<h1 id="关于监督学习、无监督学习、半监督学习、强化学习、自监督学习的区别"><a href="#关于监督学习、无监督学习、半监督学习、强化学习、自监督学习的区别" class="headerlink" title="关于监督学习、无监督学习、半监督学习、强化学习、自监督学习的区别"></a>关于监督学习、无监督学习、半监督学习、强化学习、自监督学习的区别</h1><h3 id="监督学习（Supervised-Learning-或Supervised-Machine-Learning）"><a href="#监督学习（Supervised-Learning-或Supervised-Machine-Learning）" class="headerlink" title="监督学习（Supervised Learning 或Supervised Machine Learning）"></a>监督学习（Supervised Learning 或Supervised Machine Learning）</h3><p>使用标记数据集来训练算法，一边训练后的算法可以对数据进行分类或准确预测结果。</p>
<p>可分成两类：分类（线性分类器、支持向量机、决策树、随机森林等）、回归（使用一种算法理解因变量和自变量之间的关系，有助于根据不同的数据点来预测数值）</p>
<h3 id="无监督学习（Unsupervised-Learning）"><a href="#无监督学习（Unsupervised-Learning）" class="headerlink" title="无监督学习（Unsupervised Learning）"></a>无监督学习（Unsupervised Learning）</h3><p>用算法来分析并聚类未标记的数据集，以便发现数据中隐藏的模式和规律，而不需要人工干预。</p>
<p>主要用于三个任务：聚类、关联和降维</p>
<ul>
<li>聚类（Clustering）：数据挖掘技术，用于根据未标记数据的相似性或差异性对他们进行分类分组。适用于细分市场的划分、图像压缩等。</li>
<li>关联（Association）：使用不同的规则来查找给定数据集中变量之间的关系。常用于推荐算法。</li>
<li>降维（Dimensionality Reduction）:当特定数据集中的特征（或维度）太多时，在保持数据完整性的同时，将数据输入的数量（维度）减少到可管理可操作的大小。常用于数据预处理阶段，例如用自编码器把图片数据中的噪点去除，以提高图像质量。</li>
</ul>
<h4 id="对比："><a href="#对比：" class="headerlink" title="对比："></a>对比：</h4><p>监督学习和无监督学习本质区别就是<strong>用来训练的数据是否进行标注</strong>。</p>
<p>监督学习处理数据比较耗费算力，但结果比较准确，可以解释。无监督学习处理数据算力开销不大，但是无法解释，也许是可以挖掘出未被人类注意的新规律的。</p>
<h3 id="半监督学习（Semi-supervised-Learning）"><a href="#半监督学习（Semi-supervised-Learning）" class="headerlink" title="半监督学习（Semi-supervised Learning）"></a>半监督学习（Semi-supervised Learning）</h3><p>适用情况：相对较少的标记数据+大量未标记数据</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20240508170017070.png"
                      alt="image-20240508170017070"
                ></p>
<h3 id="强化学习（Reinforcement-Learning）"><a href="#强化学习（Reinforcement-Learning）" class="headerlink" title="强化学习（Reinforcement Learning）"></a>强化学习（Reinforcement Learning）</h3><p>对算法执行的正确和不正确行为分别进行奖励和惩罚的制度，目的是使算法获得最大的累积奖励，从而学会在特定环境下做出最佳决策。</p>
<ul>
<li>代理人，Agent：一个我们试图学习的实体（即玩家在游戏中所使用的角色）；</li>
<li>环境，Environment：代理人所处的环境（游戏所设置的游戏世界设定）；</li>
<li>状态，State：代理人在环境中获得自己当前状态的各种信息；</li>
<li>行动，Actions：代理人在环境中所执行的与环境交互的各种动作（马里奥游戏中的行走、跑步、跳跃等等）；</li>
<li>奖励，Reward：代理人从环境中获得的行动反馈（在马里奥的游戏里，即为正确的行动增加的积分&#x2F;硬币，是一个积极的奖励。因落入陷阱或被怪物吃掉而丢失积分，或损失一条“命”，则是一个消极的奖励）；</li>
<li>策略，Policy：根据代理人当前的状态决定一个合适的决策，以最大化地在未来某个时间段内获得正面报酬，最小化获得负面的惩罚；</li>
<li>价值函数， Value function：决定什么才是对代理人是有益的。</li>
</ul>
<h3 id="自监督学习（self-supervised-learning）SSL"><a href="#自监督学习（self-supervised-learning）SSL" class="headerlink" title="自监督学习（self-supervised learning）SSL"></a>自监督学习（self-supervised learning）SSL</h3><p>不需要人工标注训练数据，主要训练从大规模的无监督数据中挖掘能够应用于自身的监督信息，从而从输入的一部分数据中去学习另一部分。</p>
<p>自监督学习可以通过对图片的剪裁、九宫格切割后再打乱、镜像或降低色彩饱和度等操作，让机器学会改变后的图像与原图像之间存在着十分接近的联系，这种紧密联系在二维的 Embedding 坐标空间中显示为极度靠近的坐标点。不仅仅是图片，自监督学习可以对音频、视频、文本进行同样的学习。然而这些紧密的联系，是无法通过人类标注员来操作的。就好比我们可以对图中的鸟标注为“鸟”，但是自监督学习只会把它标注为 Embedding 空间中数据结构位置信息，这在本质上和人类给这幅图标注为“鸟”是一个意思。</p>
<p>可以看出，自监督学习很容易被误解为无监督学习中的聚类，因为他们也同样是<strong>把不同的未标记的事物进行分类</strong>，但其实自监督学习是在<strong>最大化同一类样本在 Embedding 空间中表征的相似性</strong>，同时<strong>最小化不同类样本之间表征的相似性</strong>。要做到相同类别的事物表达相近，不同类别的事物表达要更远，也就是说要极端化这种对比。通过这样的极端化过程，编码器（Encoder）能学到样本在 Embedding 空间中的许多潜在特征。所谓物以类聚，人以群分！</p>
<p>可以对巨量数据自动进行更广泛的标注，对下游任务产生帮助。也适合挖掘大量的数据集中不被人类关注过的“隐蔽”信息。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20240508172136916.png"
                      alt="image-20240508172136916"
                ></p>
<h2 id="分布式训练"><a href="#分布式训练" class="headerlink" title="分布式训练"></a>分布式训练</h2><p>由于硬件资源的限制，使用多台机器共同完成训练任务。</p>
<p>(以下是突然找到了好早之前的笔记)</p>
<h2 id="解耦设计"><a href="#解耦设计" class="headerlink" title="解耦设计"></a>解耦设计</h2><p>将不同部分分离开来，以提高灵活性、可维护性和性能。</p>
<p>这种方式减少了不同部分之间的依赖关系，使他们可以更加独立地设计、实现和维护。</p>
<h2 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h2><p>作用：有效的缓解过拟合现象</p>
<p>在batch中，忽略一半的特征检测器（让一半的隐层节点值为0）。这种方式减少了特征检测器（隐层节点）之间的相互作用</p>
<p>在前向传播的时候让某个神经元的激活值以一定的概率p停止工作，这样可以使模型的泛化性更强，因为它不会太依赖某些局部的特征</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20240116011233467.png"
                      alt="image-20240116011233467"
                ></p>
<p><strong>dropout程序导致两个神经元不一定每次都在一个dropout网络中出现。这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况 。迫使网络去学习更加鲁棒的特征</strong></p>
]]></content>
      <tags>
        <tag>notes</tag>
      </tags>
  </entry>
  <entry>
    <title>PointRend__Image Segmentation as Rendering</title>
    <url>/2024/03/22/PointRend/</url>
    <content><![CDATA[<h1 id="PointRend-Image-Segmentation-as-Rendering"><a href="#PointRend-Image-Segmentation-as-Rendering" class="headerlink" title="PointRend: Image Segmentation as Rendering"></a><a class="link"   href="https://arxiv.org/abs/1912.08193" >PointRend: Image Segmentation as Rendering <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h1><p><strong>现有方法</strong>：一般会输出一个原图1&#x2F;8或1&#x2F;16大小的预测图，然后通过双线性插值来补齐最后的8x&#x2F;16x分辨率</p>
<p><strong>现有方法存在的问题</strong></p>
<p>过采样：对于图片中低频区域，没必要用太多的采样点，却使用太多采样点造成过采样；<br>欠采样 ：对于图片中高频区域（ 靠近物体边界 ），这些区域的采样过于稀疏，导致分割出的边界过于平滑。</p>
<p><strong>PointRend</strong></p>
<p>提出的 PointRend 是一个通用的模块，它允许许多可能的实现：输入一个或多个 feature map（通常比原图分辨率低 4x 或 16x），输出高分辨率的预测结果.</p>
<ul>
<li><p>PointRend 只对选择的点进行预测，而不是对所有输入点都进行预测.</p>
</li>
<li><p>对选择的点先通过插值的方式提取点特征，然后用 point head subnetwork 来预测点标签</p>
</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20240322121912139.png"
                      alt="image-20240322121912139"
                ></p>
<p><strong>Inference</strong></p>
<p>每个迭代循环中：<br>① 用双线性插值对上次预测出的 seg mask 进行上采样；<br>② 然后在上采样后的 mask 中找出 N 个最不确定的点（比如 binary mask 中 pribabilities 接近 0.5 的点）；<br>③ 用 pointrend 提取这些点的 feature 并预测它们的 label.</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20240322121856009.png"
                      alt="image-20240322121856009"
                ></p>
<p><strong>采样的三个原则</strong></p>
<p><strong>过生成</strong>: 从均匀分布中随机抽取 KN（K&gt;1）个点作为候选点。</p>
<p><strong>重要性抽样</strong>: 插值得到所有 KN 点处的的粗预测，然后计算这些点的不确定性，选择其中最不确定的 βN ( β∈[0,1] ) 个点。</p>
<p><strong>广域覆盖:</strong> 剩下的 (1−β)N 个点从均匀分布中随机采样得到</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20240322122545106.png"
                      alt="image-20240322122545106"
                ></p>
<p><strong>Coarse prediction features</strong></p>
<p><strong>必要性：</strong>只有点特征是不够的，因为 ① 对实例分割而言，一个点可能在两个实例的交叠处，需要区域信息来辅助判断；② 对实例分割和语义分割两项任务而言，点特征所使用的 feature map 可能只包含 low-level 的信息，因此还需要包含进去一些上下文和语义信息.</p>
<p>因此使用粗分割结果作为辅助信息，被涵盖在点特征表示向量中：一个点处表示 K 分类结果的 K 维向量.</p>
<p><strong>Point Head</strong></p>
<p>2 个 1024-wide hidden layers 的 MLP</p>
<p>类似于图卷积或 PointNet，这个 MLP 对所有的点<strong>共享参数.</strong></p>
]]></content>
      <tags>
        <tag>notes</tag>
      </tags>
  </entry>
  <entry>
    <title>UPDP__A Unified Progressive Depth Pruner for CNN and Vision Transformer</title>
    <url>/2024/07/01/%E5%89%AA%E6%9E%9D/</url>
    <content><![CDATA[<h1 id="UPDP-A-Unified-Progressive-Depth-Pruner-for-CNN-and-Vision-Transformer"><a href="#UPDP-A-Unified-Progressive-Depth-Pruner-for-CNN-and-Vision-Transformer" class="headerlink" title="UPDP: A Unified Progressive Depth Pruner for CNN and Vision Transformer"></a>UPDP: A Unified Progressive Depth Pruner for CNN and Vision Transformer</h1><p>简要介绍：一种很厉害的剪枝方法，适用于cnn和transformer</p>
<h2 id="传统剪枝（pruning）"><a href="#传统剪枝（pruning）" class="headerlink" title="传统剪枝（pruning）"></a><a href="https://www.cnblogs.com/armcvai/p/17143077.html"><strong>传统剪枝</strong>（pruning）</a></h2><p>模型剪枝算是模型压缩的一种，直接减少参数量，为了减少对硬件的要求、加速模型推理和落地。（模型稀疏化）</p>
<p>做法：直接删除部分不重要的权重参数，减少参数量和计算量，尽量使精度不受影响。</p>
<p>在神经网络中，<strong>非结构化稀疏</strong>包括权重稀疏、激活稀疏、梯度稀疏</p>
<h4 id="权重稀疏"><a href="#权重稀疏" class="headerlink" title="权重稀疏"></a>权重稀疏</h4><p>权重的数值分布比较像正态分布，而且越接近0，权重越多。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20240624201620571.png"
                      alt="image-20240624201620571"
                ></p>
<p>卷积层的剪枝敏感性大于全连接层，且第一层卷积层最为敏感。</p>
<p>剪枝三段式工作 <code>pipeline</code> ：训练、剪枝、微调</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20240624201850731.png"
                      alt="image-20240624201850731"
                ></p>
<p>对硬件加速不友好，尤其是GPU，因为稀疏后得到的矩阵是高度非规则的矩阵。</p>
<h4 id="激活稀疏"><a href="#激活稀疏" class="headerlink" title="激活稀疏"></a>激活稀疏</h4><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20240624205508674.png"
                      alt="image-20240624205508674"
                ></p>
<p>APoZ高  –&gt;  冗余</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20240624205700813.png"
                      alt="image-20240624205700813"
                ></p>
<p>剪枝三段式工作：</p>
<ol>
<li>正常训练，然后在大型数据集上运行网络以获得每个神经元的APoZ。</li>
<li>根据标准修剪高APoZ的神经元，并移除相应的神经元连接。</li>
<li>使用修建前的权重初始化之后再重新训练一遍。</li>
</ol>
<h3 id="结构化稀疏（粗粒系稀疏、块稀疏）"><a href="#结构化稀疏（粗粒系稀疏、块稀疏）" class="headerlink" title="结构化稀疏（粗粒系稀疏、块稀疏）"></a>结构化稀疏（粗粒系稀疏、块稀疏）</h3><ul>
<li><code>channel/filter-wise</code> 减少网络通道</li>
<li><code>shape-wise</code></li>
</ul>
<p>一般都是丢弃整行或整列的权重，或者卷积层中的整个滤波器。</p>
<p>结构化剪枝包含通道剪枝和块剪枝等技术。通道修剪侧重于消除内核内的整个通道过滤器，而块修剪则在更大范围内进行，通常针对完整块。</p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>提出一种新颖的子网块修剪策略和渐进式训练方法，而且扩展到transformer模型，效果很好</p>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>主要贡献：</p>
<p>（1）我们提出了一种统一且高效的<strong>深度剪枝方法</strong>来优化 CNN 和视觉 Transformer 模型。</p>
<p> (2)我们提出了一种用于<strong>子网优化的渐进式训练策略</strong>，以及一种<strong>使用重新参数化技术的新颖的块修剪策略</strong>。 </p>
<p>(3) 对 CNN 和视觉 Transformer 模型进行全面的实验，以展示我们的深度剪枝方法的优越剪枝性能。</p>
<p>深度卷积减少计算量和参数，但是内存占用增加</p>
<h3 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h3><p>（未完待续）</p>
]]></content>
      <tags>
        <tag>notes</tag>
      </tags>
  </entry>
  <entry>
    <title>SAM 2__Segment Anything in Images and Videos</title>
    <url>/2024/08/22/SAM2/</url>
    <content><![CDATA[<h1 id="SAM-2-Segment-Anything-in-Images-and-Videos"><a href="#SAM-2-Segment-Anything-in-Images-and-Videos" class="headerlink" title="SAM 2: Segment Anything in Images and Videos"></a>SAM 2: Segment Anything in Images and Videos</h1><h2 id="Introduction-介绍"><a href="#Introduction-介绍" class="headerlink" title="Introduction(介绍)"></a>Introduction(介绍)</h2><p>比起SAM，SAM2可以看作是SAM的拓展，从静态的图像分割，到动态的视频分割。准确度和速度都提升了。</p>
<p>（未完待续）</p>
]]></content>
      <tags>
        <tag>notes</tag>
      </tags>
  </entry>
  <entry>
    <title>RTSeg__Real-time Semantic Segmentation Comparative Study</title>
    <url>/2023/11/16/RTSeg/</url>
    <content><![CDATA[<h1 id="RTSeg-Real-time-Semantic-Segmentation-Comparative-Study"><a href="#RTSeg-Real-time-Semantic-Segmentation-Comparative-Study" class="headerlink" title="RTSeg: Real-time Semantic Segmentation Comparative Study"></a><a class="link"   href="https://arxiv.org/pdf/1803.02758v5.pdf" >RTSeg: Real-time Semantic Segmentation Comparative Study <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h1><p>这一篇更加关注计算效率，以下是一些些翻译加一些些理解。</p>
<p>针对于编码和解码模块设计出了可以灵活替换的子模块，方便大家可以方便的替换编码或者解码模块，从而针对不同任务设计不同的网络结构。</p>
<h2 id="ABSTRACT（摘要）"><a href="#ABSTRACT（摘要）" class="headerlink" title="ABSTRACT（摘要）"></a>ABSTRACT（摘要）</h2><p>Semantic segmentation benefits robotics related applications, especially autonomous driving. Most of the research on semantic segmentation only focuses on increasing the accuracy of segmentation models with little attention to computationally efficient solutions. The few work conducted in this direction does not provide principled methods to evaluate the different design choices for segmentation. In this paper, we address this gap by presenting a real-time semantic segmentation benchmarking framework with a decoupled design for feature extraction and decoding methods. The framework is comprised of different network architectures for feature extraction such as VGG16, Resnet18, MobileNet, and ShuffleNet. It is also comprised of multiple meta-architectures for segmentation that define the decoding methodology. These include SkipNet, UNet, and Dilation Frontend. Experimental results are presented on the Cityscapes dataset for urban scenes. The modular design allows novel architectures to emerge, that lead to 143x GFLOPs reduction in comparison to SegNet. This benchmarking framework is publicly available at 1 .</p>
<p>在语义分割上大多数都是提升精度，但是很少关注计算效率高的解决方案，针对这一空白，提出了一个实时语义分割基准框架，对特征提取和解码进行了解耦设计。</p>
<p>特征提取用了不同的网络结构：VGG16、Resnet18、MobileNet和ShuffleNet；解码是由多个用于分割的元架构定义的：SkipNet、UNet和Dilation Frontend。</p>
<h2 id="INTRODUCTION（介绍）"><a href="#INTRODUCTION（介绍）" class="headerlink" title="INTRODUCTION（介绍）"></a>INTRODUCTION（介绍）</h2><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20231101224230740.png"
                      alt="image-20231101224230740"
                ></p>
<p>主要贡献：</p>
<ol>
<li>将特征提取模块和解码器进行了模块化解耦，并将器成为元架构（有助于理解网络不同部分对实时性能的影响）</li>
<li>消融实验突出了精度和速度的平衡</li>
<li>我们框架的模块化设计出现了两种新颖的分割架构，分别使用MobileNet [14] 和具有多种解码方法的 ShuffleNet [15]。与 SegNet 相比，ShuffleNet 减少了 143 倍的 GFLOPs。</li>
</ol>
<h3 id="SkipNet"><a href="#SkipNet" class="headerlink" title="SkipNet"></a><a class="link"   href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Xin_Wang_SkipNet_Learning_Dynamic_ECCV_2018_paper.pdf" >SkipNet <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h3><h3 id="解码器模块介绍"><a href="#解码器模块介绍" class="headerlink" title="解码器模块介绍"></a>解码器模块介绍</h3><p>![屏幕截图 2023-11-14 172740](&#x2F;images&#x2F;屏幕截图 2023-11-14 172740.png)</p>
<p> 图2（a）是SkipNet的解码结构，类似于FCN8s的结构，其中较高高分辨率的特征图通过1x1卷积来将通道数量减少到最终的类别数量，每一个通道都对应着一个类别。</p>
<p>图2（b）是UNet的解码结构，Unet结构提供的解码方式为：利用反卷积，将与下采样阶段对应的特征图进行上采样。上采样的特征图与下采样中有相同分辨率的特征图进行融合。逐级向上采样提供的精度比一次8倍向上采样的精度更高。目前采用的融合方法是逐元素相加，concatenation的方法可以提供更高的准确率，因为其确保了网络能够学习特征的加权融合，但是这样会增加计算量(concatenation会改变通道数量)。上采样之后的特征最后会接一个1x1的卷积来输出最后的逐元素分类。</p>
<p>对于Dilation Frontend的解码结构，文中并没有给出示意图，Dilation Frontend结构利用了空洞卷积来取代下采样。空洞卷积确保了网络能够保留足够的感受野的同时，不会降低特征图的分辨率。但是副作用就是计算量的增加，修改编码器网络使得下采样率从32变为8。下采样的减少是通过删除池化层或将步幅为2的卷积转换为步幅为1的卷积来完成的。然后，将池化或正常的卷积替换为两个空洞率为2和4的空洞卷积[3]。</p>
]]></content>
      <tags>
        <tag>notes</tag>
      </tags>
  </entry>
  <entry>
    <title>SkipNet</title>
    <url>/2023/11/16/SkipNet/</url>
    <content><![CDATA[<h1 id="SkipNet"><a href="#SkipNet" class="headerlink" title="SkipNet"></a><a class="link"   href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Xin_Wang_SkipNet_Learning_Dynamic_ECCV_2018_paper.pdf" >SkipNet <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></h1><p>代码：<a class="link"   href="https://github.com/ucbdrive/skipnet" >https://github.com/ucbdrive/skipnet <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<h2 id="Abstract-（摘要）"><a href="#Abstract-（摘要）" class="headerlink" title="Abstract. （摘要）"></a>Abstract. （摘要）</h2><p>While deeper convolutional networks are needed to achieve maximum accuracy in visual perception tasks, for many inputs shallower networks are sufficient. We exploit this observation by learning to skip convolutional layers on a per-input basis. We introduce SkipNet, a modified residual network, that uses a gating network to selectively skip convolutional blocks based on the activations of the previous layer. We formulate the dynamic skipping problem in the context of sequential decision making and propose a hybrid learning algorithm that combines supervised learning and reinforcement learning to address the challenges of non-differentiable skipping decisions. We show SkipNet reduces computation by 30 − 90% while preserving the accuracy of the original model on four benchmark datasets and outperforms the state-of-the-art dynamic networks and static compression methods. We also qualitatively evaluate the gating policy to reveal a relationship between image scale and saliency and the number of layers skipped.</p>
<p>尽管需要更深的卷积网络才能在视觉感知任务中获得更大的准确性，但对于大多数输入来说，较浅的网络就够了。</p>
<p>SkipNet是经过改进的残差网络，使用gating network根据前一层的激活有选择地跳过卷积层。我们在顺序决策（sequential decision）的背景下制定动态跳过（dynamic skipping）问题，并提出一种混合学习算法，将监督学习和强化学习相结合，以解决不可微分的跳过动态的问题。</p>
<p>计算量减少了30%至90%，同时在四个基准数据集上保留了原始模型的准确性，并且胜过了最新的动态网络和静态压缩方法。</p>
<p>定性评估gating策略，以揭示图像比例和显着性（saliency）与跳过的层数之间的关系。</p>
<h2 id="Introduction（介绍）"><a href="#Introduction（介绍）" class="headerlink" title="Introduction（介绍）"></a>Introduction（介绍）</h2><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20231116225046629.png"
                      alt="image-20231116225046629"
                ></p>
<p>动态选择在推理过程中应跳过卷积神经网络的哪些层，动态跳过问题-构造-&gt;顺序决策问题，其中前一层的输出决定是否绕过后一层。</p>
<p>动态跳过问题的目标是保留整个网络准确性的同时，跳过尽可能多的层。（减少参数量，观察每层的作用）</p>
<p>为了在保持准确性的同时减少计算量，我们需要正确绕过网络中不必要的层，学习有效的跳过策略也是一项挑战。</p>
<p>（同时因为会跳过层数，所以不能应用梯度的优化）</p>
<p>过程：将gating module明确分配给每组卷积层（each group of layers）。gating模块将前一层的激活映射到二进制决策，以跳过或执行后一层。我们分两个阶段训练门控模块。首先，我们通过采用重新参数化(reparametrization)技巧对二进制<a class="link"   href="https://so.csdn.net/so/search?q=skip&spm=1001.2101.3001.7020" >skip <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>决策使用soft-max松弛，并结合原始模型使用的标准交叉熵损失一起训练layers and gates。然后，我们将概率gate输出视为初始跳过策略，并在不使用relaxation的情况下使用REINFORCE来优化策略。在后期阶段，我们共同优化跳过策略和预测误差，以稳定探索过程。</p>
<h2 id="Related-Work（相关工作）"><a href="#Related-Work（相关工作）" class="headerlink" title="Related Work（相关工作）"></a>Related Work（相关工作）</h2><p>加速现有的卷积网络：（训练初始网络后使用）权重稀疏化、滤波器修剪、矢量量化和蒸馏-&gt;将模型转移到较浅的网络上进行模型压缩</p>
<p>他人实验探索：通过提早终止来动态缩放计算、暂停循环网络以节省计算成本、在卷积网络中使用尽早终止、ResNets每组块中的提前终止</p>
<p>SkipNet并不会提早退出，而是根据处理层的输出有条件地绕过各个层，可以更好地权衡成本</p>
<h2 id="SkipNet-Model-Design（模型构思）"><a href="#SkipNet-Model-Design（模型构思）" class="headerlink" title="SkipNet Model Design（模型构思）"></a>SkipNet Model Design（模型构思）</h2><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/images/image-20231116231511966.png"
                      alt="image-20231116231511966"
                ></p>
<p>对于给定输入有选择地包括或排除了各个层。 使用插入在各层之间的小型门控网络可以完成各层的按输入选择。 gating网络将前一层或一组层的输出映射到一个二进制决策，以执行或绕过后一层或一组层</p>
<p>对于a，当gating module独立的时候，情况变复杂的时候参数量变多</p>
<h3 id="Gating-Network-Design"><a href="#Gating-Network-Design" class="headerlink" title="Gating Network Design"></a>Gating Network Design</h3>]]></content>
      <tags>
        <tag>notes</tag>
      </tags>
  </entry>
</search>
